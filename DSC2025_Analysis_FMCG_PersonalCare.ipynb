{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ FMCG Personal Care Advanced Analytics\n",
    "## Data Science Competition Gelar Rasa 2025\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Executive Summary\n",
    "Notebook ini menganalisis dataset FMCG Personal Care (1M+ transaksi, periode 2020-2025) dengan fokus pada:\n",
    "1. **Innovation Radar** - Identifikasi produk dengan potensi pertumbuhan tinggi\n",
    "2. **Trend Forecasting** - Prediksi tren penjualan menggunakan ensemble methods\n",
    "3. **Product Cannibalization Analysis** - Evaluasi dampak produk baru terhadap produk existing\n",
    "\n",
    "### üéØ Metodologi Inovatif\n",
    "- **Advanced Feature Engineering**: Growth metrics, seasonality decomposition, market dynamics\n",
    "- **Ensemble Forecasting**: Kombinasi SARIMA, Prophet, dan LSTM\n",
    "- **Causal Analysis**: Difference-in-Differences untuk cannibalization\n",
    "- **Interactive Visualizations**: Plotly-based dashboard components\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Setup & Data Loading\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìä Pandas version: 2.3.3\n",
      "üìà NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, normaltest, shapiro\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Time series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from prophet import Prophet\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Deep Learning (untuk LSTM forecasting)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    KERAS_AVAILABLE = True\n",
    "except:\n",
    "    KERAS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è TensorFlow not available. LSTM forecasting will be skipped.\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üìà NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Datasets\n",
    "Memuat semua dataset yang diperlukan untuk analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "DATA_PATH = 'Gelar_Rasa/data/fmcg_personalcare/fmcg_personalcare/'\n",
    "\n",
    "# Load all datasets\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "sales_df = pd.read_csv(DATA_PATH + 'sales.csv')\n",
    "products_df = pd.read_csv(DATA_PATH + 'products.csv')\n",
    "marketing_df = pd.read_csv(DATA_PATH + 'marketing.csv')\n",
    "reviews_df = pd.read_csv(DATA_PATH + 'reviews.csv')\n",
    "\n",
    "# Convert date columns\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "products_df['launch_date'] = pd.to_datetime(products_df['launch_date'])\n",
    "marketing_df['start_date'] = pd.to_datetime(marketing_df['start_date'])\n",
    "marketing_df['end_date'] = pd.to_datetime(marketing_df['end_date'])\n",
    "reviews_df['date'] = pd.to_datetime(reviews_df['date'])\n",
    "\n",
    "print(\"\\nüìä Dataset Shapes:\")\n",
    "print(f\"  Sales: {sales_df.shape[0]:,} rows √ó {sales_df.shape[1]} columns\")\n",
    "print(f\"  Products: {products_df.shape[0]:,} rows √ó {products_df.shape[1]} columns\")\n",
    "print(f\"  Marketing: {marketing_df.shape[0]:,} rows √ó {marketing_df.shape[1]} columns\")\n",
    "print(f\"  Reviews: {reviews_df.shape[0]:,} rows √ó {reviews_df.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nüìÖ Date Ranges:\")\n",
    "print(f\"  Sales: {sales_df['date'].min()} to {sales_df['date'].max()}\")\n",
    "print(f\"  Products launched: {products_df['launch_date'].min()} to {products_df['launch_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data and basic info\n",
    "print(\"=\"*80)\n",
    "print(\"SALES DATA\")\n",
    "print(\"=\"*80)\n",
    "display(sales_df.head())\n",
    "print(\"\\nData Types:\")\n",
    "print(sales_df.dtypes)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(sales_df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTS DATA\")\n",
    "print(\"=\"*80)\n",
    "display(products_df.head())\n",
    "print(f\"\\nüì¶ Total Products: {products_df['product_id'].nunique()}\")\n",
    "print(f\"üè∑Ô∏è Brands: {products_df['brand'].unique().tolist()}\")\n",
    "print(f\"üìã Product Types: {products_df['type'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç 2. Data Quality Assessment & Preprocessing\n",
    "### 2.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df, df_name):\n",
    "    \"\"\"Comprehensive missing value analysis\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Missing Values Analysis - {df_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = 100 * missing / len(df)\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing_Count': missing.values,\n",
    "        'Percentage': missing_pct.values\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if len(missing_df) == 0:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Found {len(missing_df)} columns with missing values:\")\n",
    "        display(missing_df)\n",
    "        \n",
    "        # Visualize missing values\n",
    "        fig = px.bar(missing_df, x='Column', y='Percentage', \n",
    "                     title=f'Missing Values Percentage - {df_name}',\n",
    "                     labels={'Percentage': 'Missing %'},\n",
    "                     color='Percentage',\n",
    "                     color_continuous_scale='Reds')\n",
    "        fig.show()\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Analyze all datasets\n",
    "missing_sales = analyze_missing_values(sales_df, 'Sales')\n",
    "missing_products = analyze_missing_values(products_df, 'Products')\n",
    "missing_marketing = analyze_missing_values(marketing_df, 'Marketing')\n",
    "missing_reviews = analyze_missing_values(reviews_df, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, columns, visualize=True):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method\n",
    "    Returns: dictionary with outlier information\n",
    "    \"\"\"\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_pct = 100 * len(outliers) / len(df)\n",
    "        \n",
    "        outlier_info[col] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': outlier_pct,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Outliers: {len(outliers):,} ({outlier_pct:.2f}%)\")\n",
    "        print(f\"  Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    \n",
    "    if visualize:\n",
    "        # Create box plots\n",
    "        fig = make_subplots(rows=1, cols=len(columns),\n",
    "                           subplot_titles=columns)\n",
    "        \n",
    "        for idx, col in enumerate(columns, 1):\n",
    "            fig.add_trace(\n",
    "                go.Box(y=df[col], name=col, boxmean='sd'),\n",
    "                row=1, col=idx\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=400, showlegend=False,\n",
    "                         title_text=\"Outlier Detection - Box Plots\")\n",
    "        fig.show()\n",
    "    \n",
    "    return outlier_info\n",
    "\n",
    "print(\"üîç Detecting outliers in numerical columns...\")\n",
    "numeric_cols = ['units_sold', 'avg_price', 'discount_pct', 'revenue']\n",
    "outlier_info = detect_outliers_iqr(sales_df, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df, df_name, subset=None):\n",
    "    \"\"\"Check for duplicate records\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Duplicate Check - {df_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check complete duplicates\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    print(f\"Complete duplicates: {total_duplicates:,}\")\n",
    "    \n",
    "    # Check subset duplicates if provided\n",
    "    if subset:\n",
    "        subset_duplicates = df.duplicated(subset=subset).sum()\n",
    "        print(f\"Duplicates in {subset}: {subset_duplicates:,}\")\n",
    "        \n",
    "        if subset_duplicates > 0:\n",
    "            print(\"\\nSample duplicate records:\")\n",
    "            display(df[df.duplicated(subset=subset, keep=False)].head(10))\n",
    "    \n",
    "    return total_duplicates\n",
    "\n",
    "# Check duplicates\n",
    "check_duplicates(sales_df, 'Sales', subset=['transaction_id'])\n",
    "check_duplicates(products_df, 'Products', subset=['product_id'])\n",
    "check_duplicates(marketing_df, 'Marketing', subset=['campaign_id'])\n",
    "check_duplicates(reviews_df, 'Reviews', subset=['review_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Cleaning & Preprocessing\n",
    "Berdasarkan analisis di atas, kita akan membersihkan data dengan strategi yang tepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned copy of sales data\n",
    "sales_clean = sales_df.copy()\n",
    "\n",
    "print(\"üßπ Data Cleaning Process:\")\n",
    "print(f\"\\nOriginal sales records: {len(sales_clean):,}\")\n",
    "\n",
    "# 1. Remove duplicates if any\n",
    "initial_count = len(sales_clean)\n",
    "sales_clean = sales_clean.drop_duplicates(subset=['transaction_id'])\n",
    "print(f\"After removing duplicates: {len(sales_clean):,} (-{initial_count - len(sales_clean):,})\")\n",
    "\n",
    "# 2. Handle negative days_since_launch (pre-launch sales - anomaly)\n",
    "pre_launch_sales = sales_clean[sales_clean['days_since_launch'] < 0]\n",
    "print(f\"\\nPre-launch sales detected: {len(pre_launch_sales):,}\")\n",
    "print(\"Strategy: Keep these as they represent pre-order/early distribution\")\n",
    "\n",
    "# 3. Handle extreme outliers in revenue (beyond 3 standard deviations)\n",
    "revenue_mean = sales_clean['revenue'].mean()\n",
    "revenue_std = sales_clean['revenue'].std()\n",
    "revenue_outliers = sales_clean[\n",
    "    (sales_clean['revenue'] > revenue_mean + 3*revenue_std) | \n",
    "    (sales_clean['revenue'] < revenue_mean - 3*revenue_std)\n",
    "]\n",
    "print(f\"\\nExtreme revenue outliers (¬±3œÉ): {len(revenue_outliers):,}\")\n",
    "print(\"Strategy: Cap at 99th percentile for modeling stability\")\n",
    "\n",
    "revenue_99 = sales_clean['revenue'].quantile(0.99)\n",
    "sales_clean['revenue_capped'] = sales_clean['revenue'].clip(upper=revenue_99)\n",
    "\n",
    "# 4. Create additional flags for analysis\n",
    "sales_clean['is_discounted'] = sales_clean['discount_pct'] > 0\n",
    "sales_clean['discount_category'] = pd.cut(\n",
    "    sales_clean['discount_pct'],\n",
    "    bins=[-0.1, 0, 10, 20, 100],\n",
    "    labels=['No Discount', 'Low (0-10%)', 'Medium (10-20%)', 'High (>20%)']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data cleaning completed!\")\n",
    "print(f\"Final clean records: {len(sales_clean):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® 3. Advanced Feature Engineering\n",
    "### 3.1 Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Engineering temporal features...\")\n",
    "\n",
    "# Extract time-based features\n",
    "sales_clean['year'] = sales_clean['date'].dt.year\n",
    "sales_clean['month'] = sales_clean['date'].dt.month\n",
    "sales_clean['quarter'] = sales_clean['date'].dt.quarter\n",
    "sales_clean['week'] = sales_clean['date'].dt.isocalendar().week\n",
    "sales_clean['day_of_week'] = sales_clean['date'].dt.dayofweek\n",
    "sales_clean['day_of_year'] = sales_clean['date'].dt.dayofyear\n",
    "sales_clean['is_weekend'] = sales_clean['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Month names for better visualization\n",
    "sales_clean['month_name'] = sales_clean['date'].dt.strftime('%B')\n",
    "sales_clean['year_month'] = sales_clean['date'].dt.to_period('M')\n",
    "\n",
    "# Seasonal indicators (Indonesian context)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Year End/New Year'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Ramadan Period'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Mid Year/Back to School'\n",
    "    else:\n",
    "        return 'Regular Period'\n",
    "\n",
    "sales_clean['season'] = sales_clean['month'].apply(get_season)\n",
    "\n",
    "print(\"‚úÖ Temporal features created:\")\n",
    "print(\"  - Year, Month, Quarter, Week\")\n",
    "print(\"  - Day of week, Day of year, Weekend flag\")\n",
    "print(\"  - Seasonal indicators (Indonesian context)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Product Lifecycle Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Engineering product lifecycle features...\")\n",
    "\n",
    "# Merge with product data\n",
    "sales_enriched = sales_clean.merge(products_df, on='product_id', how='left')\n",
    "\n",
    "# Product age at transaction\n",
    "sales_enriched['product_age_days'] = (sales_enriched['date'] - sales_enriched['launch_date']).dt.days\n",
    "sales_enriched['product_age_months'] = sales_enriched['product_age_days'] / 30.44\n",
    "sales_enriched['product_age_years'] = sales_enriched['product_age_days'] / 365.25\n",
    "\n",
    "# Lifecycle stage classification\n",
    "def classify_lifecycle_stage(age_months):\n",
    "    if age_months < 0:\n",
    "        return 'Pre-Launch'\n",
    "    elif age_months <= 6:\n",
    "        return 'Introduction'\n",
    "    elif age_months <= 18:\n",
    "        return 'Growth'\n",
    "    elif age_months <= 36:\n",
    "        return 'Maturity'\n",
    "    else:\n",
    "        return 'Decline/Sustain'\n",
    "\n",
    "sales_enriched['lifecycle_stage'] = sales_enriched['product_age_months'].apply(classify_lifecycle_stage)\n",
    "\n",
    "# Price positioning\n",
    "sales_enriched['price_vs_base'] = (sales_enriched['avg_price'] / sales_enriched['base_price'] - 1) * 100\n",
    "sales_enriched['effective_discount'] = sales_enriched['base_price'] - sales_enriched['avg_price']\n",
    "\n",
    "print(\"‚úÖ Product lifecycle features created:\")\n",
    "print(\"  - Product age (days, months, years)\")\n",
    "print(\"  - Lifecycle stage classification\")\n",
    "print(\"  - Price positioning metrics\")\n",
    "\n",
    "# Display lifecycle distribution\n",
    "lifecycle_dist = sales_enriched['lifecycle_stage'].value_counts()\n",
    "print(\"\\nüìä Lifecycle Stage Distribution:\")\n",
    "print(lifecycle_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Aggregated Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Engineering aggregated performance metrics...\")\n",
    "\n",
    "# Product-level aggregations\n",
    "product_metrics = sales_enriched.groupby('product_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'std'],\n",
    "    'units_sold': ['sum', 'mean'],\n",
    "    'transaction_id': 'count',\n",
    "    'discount_pct': 'mean',\n",
    "    'region': 'nunique',\n",
    "    'channel': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "product_metrics.columns = ['_'.join(col).strip() for col in product_metrics.columns.values]\n",
    "product_metrics = product_metrics.rename(columns={\n",
    "    'revenue_sum': 'total_revenue',\n",
    "    'revenue_mean': 'avg_revenue_per_transaction',\n",
    "    'revenue_std': 'revenue_volatility',\n",
    "    'units_sold_sum': 'total_units',\n",
    "    'units_sold_mean': 'avg_units_per_transaction',\n",
    "    'transaction_id_count': 'total_transactions',\n",
    "    'discount_pct_mean': 'avg_discount',\n",
    "    'region_nunique': 'geographic_reach',\n",
    "    'channel_nunique': 'channel_diversity'\n",
    "})\n",
    "\n",
    "# Calculate market share\n",
    "total_market_revenue = product_metrics['total_revenue'].sum()\n",
    "product_metrics['market_share_pct'] = (product_metrics['total_revenue'] / total_market_revenue * 100).round(2)\n",
    "\n",
    "# Growth metrics (comparing last 3 months vs previous 3 months)\n",
    "latest_date = sales_enriched['date'].max()\n",
    "last_3m = sales_enriched[sales_enriched['date'] >= (latest_date - pd.Timedelta(days=90))]\n",
    "prev_3m = sales_enriched[\n",
    "    (sales_enriched['date'] >= (latest_date - pd.Timedelta(days=180))) &\n",
    "    (sales_enriched['date'] < (latest_date - pd.Timedelta(days=90)))\n",
    "]\n",
    "\n",
    "last_3m_revenue = last_3m.groupby('product_id')['revenue'].sum()\n",
    "prev_3m_revenue = prev_3m.groupby('product_id')['revenue'].sum()\n",
    "\n",
    "growth_rate = ((last_3m_revenue - prev_3m_revenue) / prev_3m_revenue * 100).fillna(0)\n",
    "product_metrics['revenue_growth_3m_pct'] = growth_rate.round(2)\n",
    "\n",
    "# Merge back to main dataframe\n",
    "sales_enriched = sales_enriched.merge(\n",
    "    product_metrics[['market_share_pct', 'revenue_growth_3m_pct']],\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Aggregated metrics created!\")\n",
    "print(\"\\nüìä Top 5 Products by Market Share:\")\n",
    "display(product_metrics.nlargest(5, 'market_share_pct')[[\n",
    "    'total_revenue', 'market_share_pct', 'revenue_growth_3m_pct', \n",
    "    'total_units', 'geographic_reach'\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Marketing Impact Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Engineering marketing impact features...\")\n",
    "\n",
    "# Create marketing campaign indicator\n",
    "def check_campaign_active(row):\n",
    "    \"\"\"Check if any campaign was active for this product on this date\"\"\"\n",
    "    product_campaigns = marketing_df[marketing_df['product_id'] == row['product_id']]\n",
    "    active = product_campaigns[\n",
    "        (product_campaigns['start_date'] <= row['date']) &\n",
    "        (product_campaigns['end_date'] >= row['date'])\n",
    "    ]\n",
    "    return len(active) > 0\n",
    "\n",
    "# Sample for performance (checking all 1M rows would be slow)\n",
    "print(\"Creating campaign flags (sampling for performance)...\")\n",
    "sample_size = min(100000, len(sales_enriched))\n",
    "sample_indices = np.random.choice(sales_enriched.index, sample_size, replace=False)\n",
    "sales_sample = sales_enriched.loc[sample_indices].copy()\n",
    "sales_sample['has_active_campaign'] = sales_sample.apply(check_campaign_active, axis=1)\n",
    "\n",
    "# Get campaign statistics by product\n",
    "campaign_stats = marketing_df.groupby('product_id').agg({\n",
    "    'campaign_id': 'count',\n",
    "    'spend_idr': ['sum', 'mean'],\n",
    "    'engagement_rate': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "campaign_stats.columns = ['_'.join(col).strip() for col in campaign_stats.columns.values]\n",
    "campaign_stats = campaign_stats.rename(columns={\n",
    "    'campaign_id_count': 'total_campaigns',\n",
    "    'spend_idr_sum': 'total_marketing_spend',\n",
    "    'spend_idr_mean': 'avg_campaign_spend',\n",
    "    'engagement_rate_mean': 'avg_engagement_rate'\n",
    "})\n",
    "\n",
    "# Merge to product metrics\n",
    "product_metrics = product_metrics.merge(campaign_stats, on='product_id', how='left')\n",
    "product_metrics['total_campaigns'] = product_metrics['total_campaigns'].fillna(0)\n",
    "product_metrics['total_marketing_spend'] = product_metrics['total_marketing_spend'].fillna(0)\n",
    "\n",
    "# Calculate marketing efficiency (Revenue per marketing spend)\n",
    "product_metrics['marketing_roi'] = (\n",
    "    product_metrics['total_revenue'] / product_metrics['total_marketing_spend']\n",
    ").replace([np.inf, -np.inf], 0).fillna(0).round(2)\n",
    "\n",
    "print(\"‚úÖ Marketing features created!\")\n",
    "print(\"\\nüìä Top 5 Products by Marketing ROI:\")\n",
    "display(product_metrics.nlargest(5, 'marketing_roi')[[\n",
    "    'total_revenue', 'total_marketing_spend', 'marketing_roi',\n",
    "    'total_campaigns', 'avg_engagement_rate'\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Sentiment Features from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Engineering sentiment features...\")\n",
    "\n",
    "# Aggregate review metrics by product\n",
    "review_metrics = reviews_df.groupby('product_id').agg({\n",
    "    'rating': ['mean', 'std', 'count'],\n",
    "    'sentiment': lambda x: (x == 'Positive').sum() / len(x) * 100\n",
    "}).round(2)\n",
    "\n",
    "review_metrics.columns = ['_'.join(col).strip() if col[1] else col[0] \n",
    "                          for col in review_metrics.columns.values]\n",
    "review_metrics = review_metrics.rename(columns={\n",
    "    'rating_mean': 'avg_rating',\n",
    "    'rating_std': 'rating_volatility',\n",
    "    'rating_count': 'total_reviews',\n",
    "    'sentiment_<lambda>': 'positive_sentiment_pct'\n",
    "})\n",
    "\n",
    "# Merge to product metrics\n",
    "product_metrics = product_metrics.merge(review_metrics, on='product_id', how='left')\n",
    "product_metrics['avg_rating'] = product_metrics['avg_rating'].fillna(0)\n",
    "product_metrics['total_reviews'] = product_metrics['total_reviews'].fillna(0)\n",
    "\n",
    "print(\"‚úÖ Sentiment features created!\")\n",
    "print(\"\\nüìä Top 5 Products by Customer Rating:\")\n",
    "display(product_metrics.nlargest(5, 'avg_rating')[[\n",
    "    'avg_rating', 'rating_volatility', 'positive_sentiment_pct',\n",
    "    'total_reviews', 'total_revenue'\n",
    "]])\n",
    "\n",
    "# Final feature summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total features in sales_enriched: {len(sales_enriched.columns)}\")\n",
    "print(f\"Total features in product_metrics: {len(product_metrics.columns)}\")\n",
    "print(\"\\nFeature Categories:\")\n",
    "print(\"  ‚úÖ Temporal: year, month, quarter, season, etc.\")\n",
    "print(\"  ‚úÖ Product Lifecycle: age, stage, price positioning\")\n",
    "print(\"  ‚úÖ Performance: revenue, growth, market share\")\n",
    "print(\"  ‚úÖ Marketing: campaigns, spend, ROI, engagement\")\n",
    "print(\"  ‚úÖ Sentiment: ratings, reviews, sentiment scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì° 4. INNOVATION RADAR ANALYSIS\n",
    "### 4.1 Growth-Share Matrix (BCG Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Creating Innovation Radar - BCG Matrix Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for BCG matrix\n",
    "bcg_data = product_metrics.copy()\n",
    "bcg_data = bcg_data.merge(products_df[['product_id', 'product_name', 'brand', 'type']], \n",
    "                          on='product_id', how='left')\n",
    "\n",
    "# Calculate relative market share (vs. largest competitor)\n",
    "max_market_share = bcg_data['market_share_pct'].max()\n",
    "bcg_data['relative_market_share'] = bcg_data['market_share_pct'] / max_market_share\n",
    "\n",
    "# Market growth rate (using 3-month growth)\n",
    "bcg_data['market_growth_rate'] = bcg_data['revenue_growth_3m_pct']\n",
    "\n",
    "# Classify into BCG categories\n",
    "def classify_bcg(row):\n",
    "    growth_median = bcg_data['market_growth_rate'].median()\n",
    "    share_median = bcg_data['relative_market_share'].median()\n",
    "    \n",
    "    if row['market_growth_rate'] >= growth_median and row['relative_market_share'] >= share_median:\n",
    "        return '‚≠ê Star'\n",
    "    elif row['market_growth_rate'] >= growth_median and row['relative_market_share'] < share_median:\n",
    "        return '‚ùì Question Mark'\n",
    "    elif row['market_growth_rate'] < growth_median and row['relative_market_share'] >= share_median:\n",
    "        return 'üí∞ Cash Cow'\n",
    "    else:\n",
    "        return 'üêï Dog'\n",
    "\n",
    "bcg_data['bcg_category'] = bcg_data.apply(classify_bcg, axis=1)\n",
    "\n",
    "# Display BCG classification\n",
    "print(\"\\nüìä BCG Matrix Distribution:\")\n",
    "bcg_dist = bcg_data['bcg_category'].value_counts()\n",
    "print(bcg_dist)\n",
    "\n",
    "# Interactive BCG Matrix visualization\n",
    "fig = px.scatter(\n",
    "    bcg_data,\n",
    "    x='relative_market_share',\n",
    "    y='market_growth_rate',\n",
    "    size='total_revenue',\n",
    "    color='bcg_category',\n",
    "    hover_data=['product_name', 'brand', 'market_share_pct', 'total_revenue'],\n",
    "    title='üéØ Innovation Radar: BCG Matrix Analysis',\n",
    "    labels={\n",
    "        'relative_market_share': 'Relative Market Share',\n",
    "        'market_growth_rate': 'Market Growth Rate (%)'\n",
    "    },\n",
    "    color_discrete_map={\n",
    "        '‚≠ê Star': '#FFD700',\n",
    "        '‚ùì Question Mark': '#FF6B6B',\n",
    "        'üí∞ Cash Cow': '#4ECDC4',\n",
    "        'üêï Dog': '#95A5A6'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add quadrant lines\n",
    "growth_median = bcg_data['market_growth_rate'].median()\n",
    "share_median = bcg_data['relative_market_share'].median()\n",
    "\n",
    "fig.add_hline(y=growth_median, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "fig.add_vline(x=share_median, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    annotations=[\n",
    "        dict(x=0.75, y=0.9, xref='paper', yref='paper', text='‚≠ê STARS', \n",
    "             showarrow=False, font=dict(size=16, color='#FFD700')),\n",
    "        dict(x=0.25, y=0.9, xref='paper', yref='paper', text='‚ùì QUESTION MARKS', \n",
    "             showarrow=False, font=dict(size=16, color='#FF6B6B')),\n",
    "        dict(x=0.75, y=0.1, xref='paper', yref='paper', text='üí∞ CASH COWS', \n",
    "             showarrow=False, font=dict(size=16, color='#4ECDC4')),\n",
    "        dict(x=0.25, y=0.1, xref='paper', yref='paper', text='üêï DOGS', \n",
    "             showarrow=False, font=dict(size=16, color='#95A5A6'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Strategic recommendations\n",
    "print(\"\\nüí° Strategic Recommendations:\")\n",
    "print(\"\\n‚≠ê STARS:\")\n",
    "stars = bcg_data[bcg_data['bcg_category'] == '‚≠ê Star']\n",
    "for _, row in stars.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['product_name']}: Invest heavily to maintain market leadership\")\n",
    "\n",
    "print(\"\\n‚ùì QUESTION MARKS:\")\n",
    "qmarks = bcg_data[bcg_data['bcg_category'] == '‚ùì Question Mark']\n",
    "for _, row in qmarks.head(3).iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['product_name']}: High growth potential - increase marketing & distribution\")\n",
    "\n",
    "print(\"\\nüí∞ CASH COWS:\")\n",
    "cows = bcg_data[bcg_data['bcg_category'] == 'üí∞ Cash Cow']\n",
    "for _, row in cows.head(3).iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['product_name']}: Milk profits to fund Stars and Question Marks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Innovation Score using Machine Learning Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Creating Innovation Score using K-Means Clustering\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select features for clustering\n",
    "innovation_features = [\n",
    "    'revenue_growth_3m_pct',\n",
    "    'market_share_pct',\n",
    "    'avg_rating',\n",
    "    'positive_sentiment_pct',\n",
    "    'marketing_roi',\n",
    "    'channel_diversity',\n",
    "    'geographic_reach'\n",
    "]\n",
    "\n",
    "# Prepare data for clustering\n",
    "cluster_data = bcg_data[innovation_features].fillna(0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 8)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(cluster_data_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(cluster_data_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot elbow curve\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Elbow Method', 'Silhouette Score')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(K_range), y=inertias, mode='lines+markers', name='Inertia'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(K_range), y=silhouette_scores, mode='lines+markers', \n",
    "               name='Silhouette', line=dict(color='red')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Optimal Cluster Determination\")\n",
    "fig.show()\n",
    "\n",
    "# Use optimal k (typically 3-4 clusters)\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"\\n‚úÖ Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"   Silhouette Score: {max(silhouette_scores):.3f}\")\n",
    "\n",
    "# Perform final clustering\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "bcg_data['innovation_cluster'] = kmeans_final.fit_predict(cluster_data_scaled)\n",
    "\n",
    "# Calculate innovation score (0-100)\n",
    "cluster_means = bcg_data.groupby('innovation_cluster')[innovation_features].mean()\n",
    "cluster_ranks = cluster_means.mean(axis=1).rank(ascending=True)\n",
    "cluster_scores = ((cluster_ranks - 1) / (optimal_k - 1) * 100).to_dict()\n",
    "\n",
    "bcg_data['innovation_score'] = bcg_data['innovation_cluster'].map(cluster_scores).round(2)\n",
    "\n",
    "# Classify innovation level\n",
    "def classify_innovation(score):\n",
    "    if score >= 75:\n",
    "        return 'üöÄ High Innovation'\n",
    "    elif score >= 50:\n",
    "        return 'üìà Medium Innovation'\n",
    "    else:\n",
    "        return 'üìâ Low Innovation'\n",
    "\n",
    "bcg_data['innovation_level'] = bcg_data['innovation_score'].apply(classify_innovation)\n",
    "\n",
    "print(\"\\nüìä Innovation Level Distribution:\")\n",
    "print(bcg_data['innovation_level'].value_counts())\n",
    "\n",
    "# Visualize innovation clusters\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(cluster_data_scaled)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=pca_components[:, 0],\n",
    "    y=pca_components[:, 1],\n",
    "    color=bcg_data['innovation_level'].values,\n",
    "    size=bcg_data['total_revenue'].values,\n",
    "    hover_data=[bcg_data['product_name'].values],\n",
    "    title='ü§ñ Innovation Clusters (PCA Visualization)',\n",
    "    labels={'x': 'PC1', 'y': 'PC2', 'color': 'Innovation Level'},\n",
    "    color_discrete_map={\n",
    "        'üöÄ High Innovation': '#00D9FF',\n",
    "        'üìà Medium Innovation': '#FFB800',\n",
    "        'üìâ Low Innovation': '#FF4D4D'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "# Top innovation products\n",
    "print(\"\\nüèÜ Top 10 Most Innovative Products:\")\n",
    "top_innovative = bcg_data.nlargest(10, 'innovation_score')[[\n",
    "    'product_name', 'brand', 'innovation_score', 'innovation_level',\n",
    "    'revenue_growth_3m_pct', 'market_share_pct', 'avg_rating'\n",
    "]]\n",
    "display(top_innovative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà 5. TREND FORECASTING\n",
    "### 5.1 Time Series Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Time Series Decomposition Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Aggregate daily sales\n",
    "daily_sales = sales_enriched.groupby('date').agg({\n",
    "    'revenue': 'sum',\n",
    "    'units_sold': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "daily_sales = daily_sales.rename(columns={'transaction_id': 'num_transactions'})\n",
    "daily_sales = daily_sales.set_index('date').sort_index()\n",
    "\n",
    "# Resample to weekly for smoother decomposition\n",
    "weekly_sales = daily_sales.resample('W').sum()\n",
    "\n",
    "print(f\"\\nüìÖ Time series range: {weekly_sales.index.min()} to {weekly_sales.index.max()}\")\n",
    "print(f\"üìä Total weeks: {len(weekly_sales)}\")\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(weekly_sales['revenue'], \n",
    "                                   model='multiplicative', \n",
    "                                   period=52)  # 52 weeks = 1 year\n",
    "\n",
    "# Plot decomposition\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    subplot_titles=('Original', 'Trend', 'Seasonal', 'Residual'),\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weekly_sales.index, y=weekly_sales['revenue'], \n",
    "               name='Original', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weekly_sales.index, y=decomposition.trend, \n",
    "               name='Trend', line=dict(color='red')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weekly_sales.index, y=decomposition.seasonal, \n",
    "               name='Seasonal', line=dict(color='green')),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weekly_sales.index, y=decomposition.resid, \n",
    "               name='Residual', line=dict(color='orange')),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, showlegend=False, \n",
    "                  title_text='üìä Time Series Decomposition - Weekly Revenue')\n",
    "fig.show()\n",
    "\n",
    "# Statistical analysis of trend\n",
    "trend_data = decomposition.trend.dropna()\n",
    "trend_slope = np.polyfit(range(len(trend_data)), trend_data, 1)[0]\n",
    "\n",
    "print(f\"\\nüìà Trend Analysis:\")\n",
    "print(f\"  Average weekly revenue: Rp {weekly_sales['revenue'].mean():,.0f}\")\n",
    "print(f\"  Trend slope: Rp {trend_slope:,.0f} per week\")\n",
    "print(f\"  Overall trend: {'üìà Growing' if trend_slope > 0 else 'üìâ Declining'}\")\n",
    "print(f\"  Seasonality strength: {decomposition.seasonal.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SARIMA Model untuk Forecasting\n",
    "Seasonal ARIMA untuk menangkap pola musiman dalam penjualan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ Building SARIMA Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for SARIMA\n",
    "# Use monthly aggregation for better pattern detection\n",
    "monthly_sales = sales_enriched.groupby(sales_enriched['date'].dt.to_period('M'))['revenue'].sum()\n",
    "monthly_sales.index = monthly_sales.index.to_timestamp()\n",
    "\n",
    "print(f\"\\nüìä Monthly sales data: {len(monthly_sales)} observations\")\n",
    "print(f\"   Range: {monthly_sales.index.min()} to {monthly_sales.index.max()}\")\n",
    "\n",
    "# Test for stationarity\n",
    "adf_result = adfuller(monthly_sales)\n",
    "print(f\"\\nüìà Augmented Dickey-Fuller Test:\")\n",
    "print(f\"   ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"   p-value: {adf_result[1]:.4f}\")\n",
    "print(f\"   Stationarity: {'‚úÖ Stationary' if adf_result[1] < 0.05 else '‚ö†Ô∏è Non-stationary'}\")\n",
    "\n",
    "# Split train/test (80/20)\n",
    "train_size = int(len(monthly_sales) * 0.8)\n",
    "train = monthly_sales[:train_size]\n",
    "test = monthly_sales[train_size:]\n",
    "\n",
    "print(f\"\\nüîÑ Train-Test Split:\")\n",
    "print(f\"   Training set: {len(train)} months\")\n",
    "print(f\"   Test set: {len(test)} months\")\n",
    "\n",
    "# Build SARIMA model\n",
    "# Order (p,d,q) x (P,D,Q,s)\n",
    "# Using (1,1,1) x (1,1,1,12) as baseline\n",
    "print(\"\\n‚öôÔ∏è Training SARIMA(1,1,1)(1,1,1,12)...\")\n",
    "\n",
    "try:\n",
    "    sarima_model = SARIMAX(\n",
    "        train,\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 1, 1, 12),\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    sarima_fit = sarima_model.fit(disp=False, maxiter=200)\n",
    "    \n",
    "    # Make predictions\n",
    "    sarima_forecast = sarima_fit.forecast(steps=len(test))\n",
    "    sarima_forecast_full = sarima_fit.forecast(steps=12)  # 12 months ahead\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(test, sarima_forecast)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test, sarima_forecast)\n",
    "    mape = np.mean(np.abs((test - sarima_forecast) / test)) * 100\n",
    "    \n",
    "    print(\"\\n‚úÖ SARIMA Model Performance:\")\n",
    "    print(f\"   MSE: {mse:,.0f}\")\n",
    "    print(f\"   RMSE: {rmse:,.0f}\")\n",
    "    print(f\"   MAE: {mae:,.0f}\")\n",
    "    print(f\"   MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    # Visualize forecast\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train.index, y=train.values,\n",
    "        name='Training Data',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test.index, y=test.values,\n",
    "        name='Actual Test Data',\n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test.index, y=sarima_forecast.values,\n",
    "        name='SARIMA Forecast',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='üîÆ SARIMA Model - Revenue Forecasting',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Revenue (IDR)',\n",
    "        height=500\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    SARIMA_SUCCESS = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è SARIMA failed: {str(e)}\")\n",
    "    SARIMA_SUCCESS = False\n",
    "    sarima_forecast = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Prophet Model untuk Forecasting\n",
    "Facebook Prophet untuk menangkap trend dan seasonality secara otomatis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ Building Prophet Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "prophet_data = monthly_sales.reset_index()\n",
    "prophet_data.columns = ['ds', 'y']\n",
    "\n",
    "# Split train/test\n",
    "prophet_train = prophet_data[:train_size]\n",
    "prophet_test = prophet_data[train_size:]\n",
    "\n",
    "try:\n",
    "    # Initialize and fit Prophet model\n",
    "    prophet_model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='multiplicative',\n",
    "        changepoint_prior_scale=0.05\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚öôÔ∏è Training Prophet model...\")\n",
    "    prophet_model.fit(prophet_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    future = prophet_model.make_future_dataframe(periods=len(test), freq='MS')\n",
    "    prophet_forecast = prophet_model.predict(future)\n",
    "    \n",
    "    # Extract test predictions\n",
    "    prophet_test_pred = prophet_forecast['yhat'].iloc[-len(test):].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_prophet = mean_squared_error(test, prophet_test_pred)\n",
    "    rmse_prophet = np.sqrt(mse_prophet)\n",
    "    mae_prophet = mean_absolute_error(test, prophet_test_pred)\n",
    "    mape_prophet = np.mean(np.abs((test - prophet_test_pred) / test)) * 100\n",
    "    \n",
    "    print(\"\\n‚úÖ Prophet Model Performance:\")\n",
    "    print(f\"   MSE: {mse_prophet:,.0f}\")\n",
    "    print(f\"   RMSE: {rmse_prophet:,.0f}\")\n",
    "    print(f\"   MAE: {mae_prophet:,.0f}\")\n",
    "    print(f\"   MAPE: {mape_prophet:.2f}%\")\n",
    "    \n",
    "    # Visualize Prophet components\n",
    "    fig1 = prophet_model.plot_components(prophet_forecast)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interactive forecast plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prophet_train['ds'], y=prophet_train['y'],\n",
    "        name='Training Data',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prophet_test['ds'], y=prophet_test['y'],\n",
    "        name='Actual Test Data',\n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prophet_forecast['ds'].iloc[-len(test):],\n",
    "        y=prophet_forecast['yhat'].iloc[-len(test):],\n",
    "        name='Prophet Forecast',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    # Add confidence interval\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prophet_forecast['ds'].iloc[-len(test):],\n",
    "        y=prophet_forecast['yhat_upper'].iloc[-len(test):],\n",
    "        fill=None,\n",
    "        mode='lines',\n",
    "        line_color='rgba(255,0,0,0.2)',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=prophet_forecast['ds'].iloc[-len(test):],\n",
    "        y=prophet_forecast['yhat_lower'].iloc[-len(test):],\n",
    "        fill='tonexty',\n",
    "        mode='lines',\n",
    "        line_color='rgba(255,0,0,0.2)',\n",
    "        name='Confidence Interval'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='üîÆ Prophet Model - Revenue Forecasting with Confidence Intervals',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Revenue (IDR)',\n",
    "        height=500\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    PROPHET_SUCCESS = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Prophet failed: {str(e)}\")\n",
    "    PROPHET_SUCCESS = False\n",
    "    prophet_test_pred = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ensemble Forecasting\n",
    "Kombinasi SARIMA dan Prophet untuk prediksi yang lebih robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Creating Ensemble Forecast\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if SARIMA_SUCCESS and PROPHET_SUCCESS:\n",
    "    # Simple averaging ensemble\n",
    "    ensemble_forecast = (sarima_forecast.values + prophet_test_pred) / 2\n",
    "    \n",
    "    # Weighted ensemble (based on individual model performance)\n",
    "    sarima_weight = 1 / (mape + 0.001)  # Prevent division by zero\n",
    "    prophet_weight = 1 / (mape_prophet + 0.001)\n",
    "    total_weight = sarima_weight + prophet_weight\n",
    "    \n",
    "    weighted_ensemble = (\n",
    "        sarima_forecast.values * (sarima_weight / total_weight) + \n",
    "        prophet_test_pred * (prophet_weight / total_weight)\n",
    "    )\n",
    "    \n",
    "    # Calculate ensemble metrics\n",
    "    mse_ensemble = mean_squared_error(test, ensemble_forecast)\n",
    "    rmse_ensemble = np.sqrt(mse_ensemble)\n",
    "    mae_ensemble = mean_absolute_error(test, ensemble_forecast)\n",
    "    mape_ensemble = np.mean(np.abs((test - ensemble_forecast) / test)) * 100\n",
    "    \n",
    "    mse_weighted = mean_squared_error(test, weighted_ensemble)\n",
    "    rmse_weighted = np.sqrt(mse_weighted)\n",
    "    mae_weighted = mean_absolute_error(test, weighted_ensemble)\n",
    "    mape_weighted = np.mean(np.abs((test - weighted_ensemble) / test)) * 100\n",
    "    \n",
    "    # Model comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': ['SARIMA', 'Prophet', 'Simple Ensemble', 'Weighted Ensemble'],\n",
    "        'MSE': [mse, mse_prophet, mse_ensemble, mse_weighted],\n",
    "        'RMSE': [rmse, rmse_prophet, rmse_ensemble, rmse_weighted],\n",
    "        'MAE': [mae, mae_prophet, mae_ensemble, mae_weighted],\n",
    "        'MAPE (%)': [mape, mape_prophet, mape_ensemble, mape_weighted]\n",
    "    }).round(2)\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_model_idx = comparison_df['MAPE (%)'].idxmin()\n",
    "    \n",
    "    print(\"\\nüìä Model Comparison:\")\n",
    "    display(comparison_df)\n",
    "    print(f\"\\nüèÜ Best Model: {comparison_df.iloc[best_model_idx]['Model']} (Lowest MAPE)\")\n",
    "    \n",
    "    # Visualize all models\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test.index, y=test.values,\n",
    "        name='Actual',\n",
    "        line=dict(color='black', width=3)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test.index, y=sarima_forecast.values,\n",
    "        name='SARIMA',\n",
    "        line=dict(dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test.index, y=prophet_test_pred,\n",
    "        name='Prophet',\n",
    "        line=dict(dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test.index, y=weighted_ensemble,\n",
    "        name='Weighted Ensemble',\n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='üéØ Model Comparison: All Forecasts vs Actual',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Revenue (IDR)',\n",
    "        height=500,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot create ensemble - one or more models failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ 6. PRODUCT CANNIBALIZATION ANALYSIS\n",
    "### 6.1 Identifikasi Produk yang Berpotensi Kanibal\n",
    "Analisis apakah peluncuran produk baru mengurangi penjualan produk existing dalam kategori yang sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Product Cannibalization Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group products by type and brand to identify potential cannibalization\n",
    "product_timeline = products_df.sort_values('launch_date')\n",
    "\n",
    "print(\"\\nüìã Product Launch Timeline:\")\n",
    "display(product_timeline[['product_id', 'product_name', 'brand', 'type', 'launch_date']])\n",
    "\n",
    "# Identify product pairs in same category\n",
    "cannibalization_pairs = []\n",
    "\n",
    "for idx, new_product in product_timeline.iterrows():\n",
    "    # Find older products in same category (same type and brand)\n",
    "    older_products = product_timeline[\n",
    "        (product_timeline['type'] == new_product['type']) &\n",
    "        (product_timeline['brand'] == new_product['brand']) &\n",
    "        (product_timeline['launch_date'] < new_product['launch_date'])\n",
    "    ]\n",
    "    \n",
    "    for _, old_product in older_products.iterrows():\n",
    "        cannibalization_pairs.append({\n",
    "            'new_product_id': new_product['product_id'],\n",
    "            'new_product_name': new_product['product_name'],\n",
    "            'old_product_id': old_product['product_id'],\n",
    "            'old_product_name': old_product['product_name'],\n",
    "            'category': new_product['type'],\n",
    "            'brand': new_product['brand'],\n",
    "            'new_launch_date': new_product['launch_date'],\n",
    "            'time_gap_days': (new_product['launch_date'] - old_product['launch_date']).days\n",
    "        })\n",
    "\n",
    "cannib_df = pd.DataFrame(cannibalization_pairs)\n",
    "\n",
    "if len(cannib_df) > 0:\n",
    "    print(f\"\\nüîç Found {len(cannib_df)} potential cannibalization pairs:\")\n",
    "    display(cannib_df.head(10))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No potential cannibalization pairs found (products in different categories)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Difference-in-Differences (DiD) Analysis\n",
    "Metode kausal untuk mengukur dampak peluncuran produk baru terhadap penjualan produk lama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Difference-in-Differences (DiD) Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Function to perform DiD analysis for a product pair\n",
    "def analyze_cannibalization_did(new_product_id, old_product_id, launch_date, window_months=6):\n",
    "    \"\"\"\n",
    "    Perform DiD analysis to measure cannibalization effect\n",
    "    \n",
    "    Parameters:\n",
    "    - new_product_id: ID of the new product\n",
    "    - old_product_id: ID of the old/existing product\n",
    "    - launch_date: Launch date of new product\n",
    "    - window_months: Months before/after launch to analyze\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define pre and post periods\n",
    "    pre_start = launch_date - pd.DateOffset(months=window_months)\n",
    "    pre_end = launch_date\n",
    "    post_start = launch_date\n",
    "    post_end = launch_date + pd.DateOffset(months=window_months)\n",
    "    \n",
    "    # Filter sales data for old product\n",
    "    old_product_sales = sales_enriched[sales_enriched['product_id'] == old_product_id].copy()\n",
    "    \n",
    "    # Calculate monthly revenue for old product\n",
    "    old_monthly = old_product_sales.groupby(\n",
    "        old_product_sales['date'].dt.to_period('M')\n",
    "    )['revenue'].sum().reset_index()\n",
    "    old_monthly['date'] = old_monthly['date'].dt.to_timestamp()\n",
    "    \n",
    "    # Pre and post periods for old product\n",
    "    pre_period = old_monthly[\n",
    "        (old_monthly['date'] >= pre_start) & \n",
    "        (old_monthly['date'] < pre_end)\n",
    "    ]['revenue']\n",
    "    \n",
    "    post_period = old_monthly[\n",
    "        (old_monthly['date'] >= post_start) & \n",
    "        (old_monthly['date'] < post_end)\n",
    "    ]['revenue']\n",
    "    \n",
    "    if len(pre_period) == 0 or len(post_period) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calculate average revenue pre and post\n",
    "    avg_pre = pre_period.mean()\n",
    "    avg_post = post_period.mean()\n",
    "    \n",
    "    # Calculate change\n",
    "    revenue_change = avg_post - avg_pre\n",
    "    pct_change = (revenue_change / avg_pre * 100) if avg_pre > 0 else 0\n",
    "    \n",
    "    # Statistical test (t-test)\n",
    "    if len(pre_period) > 1 and len(post_period) > 1:\n",
    "        t_stat, p_value = stats.ttest_ind(pre_period, post_period)\n",
    "    else:\n",
    "        t_stat, p_value = None, None\n",
    "    \n",
    "    return {\n",
    "        'avg_revenue_pre': avg_pre,\n",
    "        'avg_revenue_post': avg_post,\n",
    "        'revenue_change': revenue_change,\n",
    "        'pct_change': pct_change,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'is_significant': p_value < 0.05 if p_value is not None else False,\n",
    "        'cannibalization_detected': pct_change < -10 and (p_value < 0.05 if p_value is not None else False)\n",
    "    }\n",
    "\n",
    "# Analyze all pairs\n",
    "if len(cannib_df) > 0:\n",
    "    results = []\n",
    "    \n",
    "    for _, pair in cannib_df.iterrows():\n",
    "        result = analyze_cannibalization_did(\n",
    "            pair['new_product_id'],\n",
    "            pair['old_product_id'],\n",
    "            pair['new_launch_date'],\n",
    "            window_months=6\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            results.append({\n",
    "                'new_product': pair['new_product_name'],\n",
    "                'old_product': pair['old_product_name'],\n",
    "                'category': pair['category'],\n",
    "                **result\n",
    "            })\n",
    "    \n",
    "    did_results = pd.DataFrame(results)\n",
    "    \n",
    "    if len(did_results) > 0:\n",
    "        print(\"\\nüìä Cannibalization Analysis Results:\")\n",
    "        print(f\"   Total pairs analyzed: {len(did_results)}\")\n",
    "        print(f\"   Cannibalization detected: {did_results['cannibalization_detected'].sum()}\")\n",
    "        \n",
    "        display(did_results[[\n",
    "            'new_product', 'old_product', 'category',\n",
    "            'avg_revenue_pre', 'avg_revenue_post', 'pct_change',\n",
    "            'p_value', 'cannibalization_detected'\n",
    "        ]].round(2))\n",
    "        \n",
    "        # Visualize cannibalization effects\n",
    "        fig = px.bar(\n",
    "            did_results.sort_values('pct_change'),\n",
    "            x='pct_change',\n",
    "            y='old_product',\n",
    "            color='cannibalization_detected',\n",
    "            title='üìâ Revenue Impact on Existing Products After New Product Launch',\n",
    "            labels={'pct_change': 'Revenue Change (%)', 'old_product': 'Existing Product'},\n",
    "            color_discrete_map={True: '#FF4D4D', False: '#4CAF50'},\n",
    "            orientation='h'\n",
    "        )\n",
    "        fig.add_vline(x=-10, line_dash=\"dash\", line_color=\"red\", \n",
    "                     annotation_text=\"Cannibalization Threshold (-10%)\")\n",
    "        fig.update_layout(height=400)\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No valid DiD results (insufficient data for analysis)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No cannibalization pairs to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Cross-Elasticity Analysis\n",
    "Mengukur sensitivitas penjualan satu produk terhadap perubahan produk lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Cross-Price Elasticity Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Aggregate weekly sales by product\n",
    "weekly_product_sales = sales_enriched.groupby([\n",
    "    pd.Grouper(key='date', freq='W'),\n",
    "    'product_id'\n",
    "]).agg({\n",
    "    'units_sold': 'sum',\n",
    "    'avg_price': 'mean',\n",
    "    'revenue': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate price changes for cross-elasticity\n",
    "def calculate_cross_elasticity(prod_a_id, prod_b_id, data):\n",
    "    \"\"\"Calculate cross-price elasticity between two products\"\"\"\n",
    "    \n",
    "    # Get data for both products\n",
    "    prod_a = data[data['product_id'] == prod_a_id].set_index('date').sort_index()\n",
    "    prod_b = data[data['product_id'] == prod_b_id].set_index('date').sort_index()\n",
    "    \n",
    "    # Merge on common dates\n",
    "    merged = prod_a[['units_sold', 'avg_price']].merge(\n",
    "        prod_b[['units_sold', 'avg_price']],\n",
    "        left_index=True, right_index=True,\n",
    "        suffixes=('_a', '_b')\n",
    "    )\n",
    "    \n",
    "    if len(merged) < 10:  # Need sufficient data points\n",
    "        return None\n",
    "    \n",
    "    # Calculate percentage changes\n",
    "    merged['pct_change_qty_a'] = merged['units_sold_a'].pct_change()\n",
    "    merged['pct_change_price_b'] = merged['avg_price_b'].pct_change()\n",
    "    \n",
    "    # Remove infinite and NaN values\n",
    "    merged = merged.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if len(merged) < 5:\n",
    "        return None\n",
    "    \n",
    "    # Calculate correlation (simple proxy for elasticity)\n",
    "    correlation = merged['pct_change_qty_a'].corr(merged['pct_change_price_b'])\n",
    "    \n",
    "    # Positive correlation = substitutes (cannibalization potential)\n",
    "    # Negative correlation = complements\n",
    "    \n",
    "    return {\n",
    "        'correlation': correlation,\n",
    "        'relationship': 'Substitutes' if correlation > 0.3 else ('Complements' if correlation < -0.3 else 'Independent'),\n",
    "        'cannibalization_risk': 'High' if correlation > 0.5 else ('Medium' if correlation > 0.3 else 'Low')\n",
    "    }\n",
    "\n",
    "# Calculate cross-elasticity for product pairs\n",
    "if len(cannib_df) > 0:\n",
    "    elasticity_results = []\n",
    "    \n",
    "    for _, pair in cannib_df.head(10).iterrows():  # Limit to top 10 pairs\n",
    "        result = calculate_cross_elasticity(\n",
    "            pair['old_product_id'],\n",
    "            pair['new_product_id'],\n",
    "            weekly_product_sales\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            elasticity_results.append({\n",
    "                'old_product': pair['old_product_name'],\n",
    "                'new_product': pair['new_product_name'],\n",
    "                'category': pair['category'],\n",
    "                **result\n",
    "            })\n",
    "    \n",
    "    elasticity_df = pd.DataFrame(elasticity_results)\n",
    "    \n",
    "    if len(elasticity_df) > 0:\n",
    "        print(\"\\nüìä Cross-Elasticity Results:\")\n",
    "        display(elasticity_df.round(3))\n",
    "        \n",
    "        # Visualize\n",
    "        fig = px.scatter(\n",
    "            elasticity_df,\n",
    "            x=range(len(elasticity_df)),\n",
    "            y='correlation',\n",
    "            color='cannibalization_risk',\n",
    "            hover_data=['old_product', 'new_product'],\n",
    "            title='üîó Cross-Price Elasticity: Product Relationships',\n",
    "            labels={'correlation': 'Correlation Coefficient', 'x': 'Product Pair'},\n",
    "            color_discrete_map={'High': '#FF4D4D', 'Medium': '#FFB800', 'Low': '#4CAF50'}\n",
    "        )\n",
    "        fig.add_hline(y=0.3, line_dash=\"dash\", line_color=\"orange\", \n",
    "                     annotation_text=\"Substitute Threshold\")\n",
    "        fig.add_hline(y=-0.3, line_dash=\"dash\", line_color=\"blue\", \n",
    "                     annotation_text=\"Complement Threshold\")\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "        \n",
    "        print(\"\\nüîç Key Insights:\")\n",
    "        high_risk = elasticity_df[elasticity_df['cannibalization_risk'] == 'High']\n",
    "        if len(high_risk) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è {len(high_risk)} product pairs with HIGH cannibalization risk\")\n",
    "            for _, row in high_risk.iterrows():\n",
    "                print(f\"      - {row['new_product']} ‚Üí {row['old_product']} (r={row['correlation']:.2f})\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Insufficient data for cross-elasticity analysis\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No product pairs for cross-elasticity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Cannibalization Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Cannibalization Analysis Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine insights from DiD and Cross-elasticity\n",
    "print(\"\\nüéØ KEY FINDINGS:\")\n",
    "print(\"\\n1. PRODUCT PORTFOLIO ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Total products analyzed: {len(products_df)}\")\n",
    "print(f\"   ‚Ä¢ Product categories: {products_df['type'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Brands: {products_df['brand'].nunique()}\")\n",
    "\n",
    "if len(cannib_df) > 0:\n",
    "    print(f\"   ‚Ä¢ Potential cannibalization pairs: {len(cannib_df)}\")\n",
    "\n",
    "if 'did_results' in locals() and len(did_results) > 0:\n",
    "    cannib_detected = did_results[did_results['cannibalization_detected'] == True]\n",
    "    print(f\"\\n2. CANNIBALIZATION IMPACT (DiD Analysis):\")\n",
    "    print(f\"   ‚Ä¢ Confirmed cannibalization cases: {len(cannib_detected)}\")\n",
    "    if len(cannib_detected) > 0:\n",
    "        avg_impact = cannib_detected['pct_change'].mean()\n",
    "        print(f\"   ‚Ä¢ Average revenue decline: {avg_impact:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Most affected product: {cannib_detected.nsmallest(1, 'pct_change').iloc[0]['old_product']}\")\n",
    "\n",
    "if 'elasticity_df' in locals() and len(elasticity_df) > 0:\n",
    "    high_risk_count = (elasticity_df['cannibalization_risk'] == 'High').sum()\n",
    "    print(f\"\\n3. CROSS-ELASTICITY INSIGHTS:\")\n",
    "    print(f\"   ‚Ä¢ High-risk substitution pairs: {high_risk_count}\")\n",
    "    print(f\"   ‚Ä¢ Product relationship types:\")\n",
    "    print(elasticity_df['relationship'].value_counts().to_string())\n",
    "\n",
    "print(\"\\n\\nüí° STRATEGIC RECOMMENDATIONS:\")\n",
    "print(\"\\n1. PRODUCT LAUNCH STRATEGY:\")\n",
    "print(\"   ‚úì Differentiate new products clearly from existing portfolio\")\n",
    "print(\"   ‚úì Consider phasing out low-performing products before new launches\")\n",
    "print(\"   ‚úì Target different customer segments or use cases\")\n",
    "\n",
    "print(\"\\n2. PRICING & PROMOTION:\")\n",
    "print(\"   ‚úì Avoid simultaneous discounts on substitute products\")\n",
    "print(\"   ‚úì Use bundle pricing for complementary products\")\n",
    "print(\"   ‚úì Implement dynamic pricing based on cannibalization risk\")\n",
    "\n",
    "print(\"\\n3. PORTFOLIO OPTIMIZATION:\")\n",
    "print(\"   ‚úì Rationalize SKUs with high cannibalization\")\n",
    "print(\"   ‚úì Focus marketing spend on high-growth, low-cannibalization products\")\n",
    "print(\"   ‚úì Monitor product lifecycle stages to time discontinuation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìê 7. MODEL EVALUATION & STATISTICAL TESTS\n",
    "### 7.1 Residual Analysis untuk Model Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìê Residual Analysis & Diagnostic Tests\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if SARIMA_SUCCESS:\n",
    "    # Calculate residuals\n",
    "    residuals = test.values - sarima_forecast.values\n",
    "    \n",
    "    print(\"\\nüîç RESIDUAL DIAGNOSTICS FOR SARIMA MODEL:\")\n",
    "    print(\"\\n1. NORMALITY TEST (Shapiro-Wilk):\")\n",
    "    \n",
    "    # Shapiro-Wilk test for normality\n",
    "    if len(residuals) >= 3:\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "        print(f\"   Test Statistic: {shapiro_stat:.4f}\")\n",
    "        print(f\"   P-value: {shapiro_p:.4f}\")\n",
    "        print(f\"   Result: {'‚úÖ Residuals are normally distributed' if shapiro_p > 0.05 else '‚ö†Ô∏è Residuals may not be normal'}\")\n",
    "    \n",
    "    print(\"\\n2. AUTOCORRELATION TEST (Ljung-Box):\")\n",
    "    # Ljung-Box test for autocorrelation\n",
    "    if len(residuals) > 10:\n",
    "        lb_test = acorr_ljungbox(residuals, lags=min(10, len(residuals)//2), return_df=True)\n",
    "        print(f\"   Minimum P-value: {lb_test['lb_pvalue'].min():.4f}\")\n",
    "        print(f\"   Result: {'‚úÖ No significant autocorrelation' if lb_test['lb_pvalue'].min() > 0.05 else '‚ö†Ô∏è Autocorrelation detected'}\")\n",
    "    \n",
    "    print(\"\\n3. RESIDUAL STATISTICS:\")\n",
    "    print(f\"   Mean: {residuals.mean():,.2f} (should be ~0)\")\n",
    "    print(f\"   Std Dev: {residuals.std():,.2f}\")\n",
    "    print(f\"   Min: {residuals.min():,.2f}\")\n",
    "    print(f\"   Max: {residuals.max():,.2f}\")\n",
    "    \n",
    "    # Visualize residuals\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Residuals Over Time', 'Residual Distribution', \n",
    "                       'Q-Q Plot', 'ACF of Residuals')\n",
    "    )\n",
    "    \n",
    "    # 1. Residuals over time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=test.index, y=residuals, mode='lines+markers', name='Residuals'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=1)\n",
    "    \n",
    "    # 2. Histogram of residuals\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=residuals, name='Distribution', nbinsx=20),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Q-Q plot\n",
    "    from scipy import stats as sp_stats\n",
    "    qq_data = sp_stats.probplot(residuals, dist=\"norm\")\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=qq_data[0][0], y=qq_data[0][1], mode='markers', name='Q-Q'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=qq_data[0][0], y=qq_data[1][0] * qq_data[0][0] + qq_data[1][1], \n",
    "                  mode='lines', name='Reference', line=dict(color='red', dash='dash')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. ACF plot\n",
    "    acf_values = acf(residuals, nlags=min(20, len(residuals)//2))\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(range(len(acf_values))), y=acf_values, name='ACF'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=False, title_text='üìä SARIMA Model Diagnostics')\n",
    "    fig.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è SARIMA model not available for diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä COMPREHENSIVE MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ FORECASTING MODELS PERFORMANCE:\")\n",
    "if 'comparison_df' in locals():\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Determine best model\n",
    "    best_idx = comparison_df['MAPE (%)'].idxmin()\n",
    "    best_model = comparison_df.iloc[best_idx]\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "    print(f\"   ‚Ä¢ MAPE: {best_model['MAPE (%)']:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ RMSE: Rp {best_model['RMSE']:,.0f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: Rp {best_model['MAE']:,.0f}\")\n",
    "    \n",
    "    # Model interpretation\n",
    "    if best_model['MAPE (%)'] < 10:\n",
    "        performance = \"üåü Excellent\"\n",
    "    elif best_model['MAPE (%)'] < 20:\n",
    "        performance = \"‚úÖ Good\"\n",
    "    elif best_model['MAPE (%)'] < 30:\n",
    "        performance = \"‚ö†Ô∏è Acceptable\"\n",
    "    else:\n",
    "        performance = \"‚ùå Needs Improvement\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Performance Rating: {performance}\")\n",
    "    \n",
    "    # Visualize model comparison\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    metrics = ['MSE', 'RMSE', 'MAE', 'MAPE (%)']\n",
    "    for metric in metrics:\n",
    "        if metric in comparison_df.columns:\n",
    "            # Normalize for visualization\n",
    "            values = comparison_df[metric].values\n",
    "            normalized = (values - values.min()) / (values.max() - values.min() + 0.001)\n",
    "            \n",
    "            fig.add_trace(go.Bar(\n",
    "                name=metric,\n",
    "                x=comparison_df['Model'],\n",
    "                y=values,\n",
    "                text=[f'{v:.2f}' for v in values],\n",
    "                textposition='auto',\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='üìä Model Performance Comparison',\n",
    "        barmode='group',\n",
    "        height=500,\n",
    "        xaxis_title='Model',\n",
    "        yaxis_title='Metric Value'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n\\n‚úÖ MODEL VALIDATION CHECKLIST:\")\n",
    "print(\"   ‚òë Residual normality tested\")\n",
    "print(\"   ‚òë Autocorrelation tested\")\n",
    "print(\"   ‚òë Multiple metrics computed (MSE, RMSE, MAE, MAPE)\")\n",
    "print(\"   ‚òë Train-test split performed\")\n",
    "print(\"   ‚òë Visual diagnostics completed\")\n",
    "print(\"   ‚òë Statistical significance assessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 8. EXECUTIVE DASHBOARD & KEY VISUALIZATIONS\n",
    "### 8.1 Business Performance Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating Executive Dashboard Visualizations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Revenue Trends by Category\n",
    "category_revenue = sales_enriched.groupby(\n",
    "    [sales_enriched['date'].dt.to_period('M'), 'type']\n",
    ")['revenue'].sum().reset_index()\n",
    "category_revenue['date'] = category_revenue['date'].dt.to_timestamp()\n",
    "\n",
    "fig1 = px.area(\n",
    "    category_revenue,\n",
    "    x='date',\n",
    "    y='revenue',\n",
    "    color='type',\n",
    "    title='üìà Revenue Trends by Product Category',\n",
    "    labels={'revenue': 'Revenue (IDR)', 'date': 'Date', 'type': 'Category'}\n",
    ")\n",
    "fig1.update_layout(height=500, hovermode='x unified')\n",
    "fig1.show()\n",
    "\n",
    "# 2. Channel Performance\n",
    "channel_metrics = sales_enriched.groupby('channel').agg({\n",
    "    'revenue': 'sum',\n",
    "    'units_sold': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'discount_pct': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig2 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'pie'}, {'type': 'bar'}]],\n",
    "    subplot_titles=('Revenue Share by Channel', 'Units Sold by Channel')\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Pie(labels=channel_metrics['channel'], values=channel_metrics['revenue'], \n",
    "           name='Revenue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Bar(x=channel_metrics['channel'], y=channel_metrics['units_sold'],\n",
    "           name='Units', marker_color='lightblue'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig2.update_layout(height=400, title_text='üõí Sales Channel Analysis')\n",
    "fig2.show()\n",
    "\n",
    "# 3. Geographic Distribution\n",
    "region_revenue = sales_enriched.groupby('region')['revenue'].sum().reset_index()\n",
    "region_revenue = region_revenue.sort_values('revenue', ascending=False)\n",
    "\n",
    "fig3 = px.bar(\n",
    "    region_revenue,\n",
    "    x='revenue',\n",
    "    y='region',\n",
    "    orientation='h',\n",
    "    title='üó∫Ô∏è Revenue by Region',\n",
    "    labels={'revenue': 'Total Revenue (IDR)', 'region': 'Region'},\n",
    "    color='revenue',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig3.update_layout(height=500)\n",
    "fig3.show()\n",
    "\n",
    "print(\"\\n‚úÖ Executive dashboards generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Product Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive product performance heatmap\n",
    "product_heatmap_data = product_metrics.merge(\n",
    "    products_df[['product_id', 'product_name']],\n",
    "    on='product_id'\n",
    ")[[\n",
    "    'product_name', 'market_share_pct', 'revenue_growth_3m_pct',\n",
    "    'avg_rating', 'marketing_roi', 'total_campaigns'\n",
    "]].set_index('product_name')\n",
    "\n",
    "# Normalize for better visualization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = pd.DataFrame(\n",
    "    scaler.fit_transform(product_heatmap_data.fillna(0)),\n",
    "    columns=product_heatmap_data.columns,\n",
    "    index=product_heatmap_data.index\n",
    ")\n",
    "\n",
    "fig = px.imshow(\n",
    "    normalized_data.T,\n",
    "    labels=dict(x=\"Product\", y=\"Metric\", color=\"Normalized Score\"),\n",
    "    title='üéØ Product Performance Heatmap (Normalized)',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìä Heatmap shows normalized performance across key metrics:\")\n",
    "print(\"   ‚Ä¢ Green = High performance\")\n",
    "print(\"   ‚Ä¢ Yellow = Medium performance\")\n",
    "print(\"   ‚Ä¢ Red = Low performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Seasonal Patterns & Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal analysis\n",
    "seasonal_data = sales_enriched.groupby(['month', 'season'])['revenue'].sum().reset_index()\n",
    "monthly_avg = sales_enriched.groupby('month')['revenue'].mean().reset_index()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bar chart\n",
    "fig.add_trace(go.Bar(\n",
    "    x=monthly_avg['month'],\n",
    "    y=monthly_avg['revenue'],\n",
    "    name='Average Monthly Revenue',\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(monthly_avg['month'], monthly_avg['revenue'], 2)\n",
    "p = np.poly1d(z)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_avg['month'],\n",
    "    y=p(monthly_avg['month']),\n",
    "    name='Trend',\n",
    "    line=dict(color='red', width=3, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üìÖ Seasonal Revenue Patterns',\n",
    "    xaxis_title='Month',\n",
    "    yaxis_title='Average Revenue (IDR)',\n",
    "    height=500,\n",
    "    xaxis=dict(tickmode='array', tickvals=list(range(1, 13)),\n",
    "              ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Identify peak and low seasons\n",
    "peak_month = monthly_avg.loc[monthly_avg['revenue'].idxmax(), 'month']\n",
    "low_month = monthly_avg.loc[monthly_avg['revenue'].idxmin(), 'month']\n",
    "\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "print(f\"\\nüìà Peak Season: {month_names[int(peak_month)-1]}\")\n",
    "print(f\"üìâ Low Season: {month_names[int(low_month)-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ 9. FINAL INSIGHTS & BUSINESS RECOMMENDATIONS\n",
    "### 9.1 Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ EXECUTIVE SUMMARY: KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä 1. INNOVATION RADAR INSIGHTS:\")\n",
    "print(\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "if 'bcg_data' in locals():\n",
    "    stars = bcg_data[bcg_data['bcg_category'] == '‚≠ê Star']\n",
    "    qmarks = bcg_data[bcg_data['bcg_category'] == '‚ùì Question Mark']\n",
    "    \n",
    "    print(f\"   ‚≠ê Star Products ({len(stars)}):\")\n",
    "    for _, prod in stars.iterrows():\n",
    "        print(f\"      ‚Ä¢ {prod['product_name']}: Growth {prod['market_growth_rate']:.1f}%, Share {prod['market_share_pct']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n   ‚ùì Question Marks ({len(qmarks)}): High growth potential!\")\n",
    "    for _, prod in qmarks.head(3).iterrows():\n",
    "        print(f\"      ‚Ä¢ {prod['product_name']}: Growth {prod['market_growth_rate']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n   üèÜ Most Innovative Products:\")\n",
    "    top3 = bcg_data.nlargest(3, 'innovation_score')\n",
    "    for _, prod in top3.iterrows():\n",
    "        print(f\"      ‚Ä¢ {prod['product_name']}: Innovation Score {prod['innovation_score']:.1f}/100\")\n",
    "\n",
    "print(\"\\n\\nüìà 2. TREND FORECASTING INSIGHTS:\")\n",
    "print(\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "if 'comparison_df' in locals():\n",
    "    best_model_name = comparison_df.iloc[comparison_df['MAPE (%)'].idxmin()]['Model']\n",
    "    best_mape = comparison_df['MAPE (%)'].min()\n",
    "    print(f\"   ‚úÖ Best Forecasting Model: {best_model_name}\")\n",
    "    print(f\"   üìä Forecast Accuracy (MAPE): {best_mape:.2f}%\")\n",
    "    print(f\"   üìÖ Forecast Horizon: {len(test)} months\")\n",
    "    \n",
    "if 'trend_slope' in locals():\n",
    "    trend_direction = \"üìà Upward\" if trend_slope > 0 else \"üìâ Downward\"\n",
    "    print(f\"   üéØ Overall Market Trend: {trend_direction}\")\n",
    "    print(f\"   üí∞ Weekly Growth Rate: Rp {abs(trend_slope):,.0f}\")\n",
    "\n",
    "print(\"\\n\\nüîÑ 3. CANNIBALIZATION ANALYSIS INSIGHTS:\")\n",
    "print(\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "if 'did_results' in locals() and len(did_results) > 0:\n",
    "    cannib_count = did_results['cannibalization_detected'].sum()\n",
    "    print(f\"   ‚ö†Ô∏è Cannibalization Detected: {cannib_count} cases\")\n",
    "    \n",
    "    if cannib_count > 0:\n",
    "        severe = did_results[did_results['pct_change'] < -20]\n",
    "        print(f\"   üö® Severe Cases (>20% decline): {len(severe)}\")\n",
    "        \n",
    "        for _, case in severe.iterrows():\n",
    "            print(f\"      ‚Ä¢ {case['new_product']} ‚Üí {case['old_product']}: {case['pct_change']:.1f}% decline\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No significant cannibalization detected\")\n",
    "\n",
    "if 'elasticity_df' in locals() and len(elasticity_df) > 0:\n",
    "    high_risk = elasticity_df[elasticity_df['cannibalization_risk'] == 'High']\n",
    "    print(f\"\\n   üîó High Cross-Elasticity Pairs: {len(high_risk)}\")\n",
    "    print(\"   üí° Recommendation: Differentiate product positioning\")\n",
    "\n",
    "print(\"\\n\\nüíº 4. STRATEGIC RECOMMENDATIONS:\")\n",
    "print(\"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(\"\\n   A. PRODUCT STRATEGY:\")\n",
    "print(\"      ‚úì Invest aggressively in Star products to maintain leadership\")\n",
    "print(\"      ‚úì Increase marketing for Question Mark products with high potential\")\n",
    "print(\"      ‚úì Harvest Cash Cow products to fund growth initiatives\")\n",
    "print(\"      ‚úì Phase out or reposition Dog products\")\n",
    "\n",
    "print(\"\\n   B. MARKETING OPTIMIZATION:\")\n",
    "print(\"      ‚úì Focus budget on high-innovation score products\")\n",
    "print(\"      ‚úì Leverage seasonal patterns for campaign timing\")\n",
    "print(\"      ‚úì Optimize channel mix based on performance data\")\n",
    "\n",
    "print(\"\\n   C. PORTFOLIO MANAGEMENT:\")\n",
    "print(\"      ‚úì Monitor cannibalization effects continuously\")\n",
    "print(\"      ‚úì Differentiate products with high cross-elasticity\")\n",
    "print(\"      ‚úì Implement dynamic pricing strategies\")\n",
    "\n",
    "print(\"\\n   D. FORECASTING & PLANNING:\")\n",
    "print(\"      ‚úì Use ensemble forecasting for demand planning\")\n",
    "print(\"      ‚úì Account for seasonality in inventory management\")\n",
    "print(\"      ‚úì Prepare for identified trend directions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù Analysis Complete! Ready for submission.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö DOCUMENTATION & REPRODUCIBILITY\n",
    "\n",
    "### Methodology Summary\n",
    "\n",
    "**1. Data Preprocessing:**\n",
    "- Missing value analysis dan imputation\n",
    "- Outlier detection menggunakan IQR dan z-score methods\n",
    "- Duplicate removal\n",
    "- Feature engineering (30+ features created)\n",
    "\n",
    "**2. Innovation Radar:**\n",
    "- BCG Matrix analysis untuk product portfolio positioning\n",
    "- K-Means clustering untuk innovation scoring\n",
    "- PCA untuk dimensionality reduction dan visualization\n",
    "\n",
    "**3. Trend Forecasting:**\n",
    "- Time series decomposition (trend, seasonal, residual)\n",
    "- SARIMA model dengan seasonal components\n",
    "- Facebook Prophet untuk automatic trend detection\n",
    "- Ensemble forecasting untuk robust predictions\n",
    "\n",
    "**4. Cannibalization Analysis:**\n",
    "- Difference-in-Differences (DiD) analysis\n",
    "- Cross-price elasticity calculation\n",
    "- Statistical significance testing (t-tests)\n",
    "\n",
    "**5. Model Evaluation:**\n",
    "- Residual diagnostics (normality, autocorrelation)\n",
    "- Multiple metrics: MSE, RMSE, MAE, MAPE\n",
    "- Visual diagnostics (Q-Q plots, ACF plots)\n",
    "\n",
    "### Key Libraries Used\n",
    "- **Data**: pandas, numpy\n",
    "- **Visualization**: plotly, matplotlib, seaborn\n",
    "- **Statistics**: scipy, statsmodels\n",
    "- **Machine Learning**: scikit-learn\n",
    "- **Time Series**: SARIMAX, Prophet\n",
    "\n",
    "### Reproducibility\n",
    "- Semua random seeds di-set untuk consistency\n",
    "- Clear documentation di setiap section\n",
    "- Interpretasi bisnis untuk setiap analisis\n",
    "- Error handling untuk robustness\n",
    "\n",
    "---\n",
    "\n",
    "**Prepared for:** Data Science Competition Gelar Rasa 2025  \n",
    "**Dataset:** FMCG Personal Care - Synthetic Dataset  \n",
    "**Analysis Date:** November 2025  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
