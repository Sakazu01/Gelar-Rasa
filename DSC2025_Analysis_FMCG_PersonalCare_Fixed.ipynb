{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Data FMCG Personal Care\n",
    "## Data Science Competition - Gelar Rasa 2025\n",
    "\n",
    "**Objective:** Menganalisis dataset produk FMCG Personal Care untuk mendapatkan insight mendalam terkait:\n",
    "- Innovation Radar: Positioning produk berdasarkan inovasi\n",
    "- Trend Forecasting: Prediksi tren penjualan masa depan\n",
    "- Product Cannibalization: Identifikasi kompetisi internal antar produk\n",
    "\n",
    "**Dataset:** Sales, Products, Marketing, Reviews\n",
    "\n",
    "**Approach:** End-to-end data science pipeline dari ingestion hingga business insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "Random Seed: 42\n",
      "\n",
      "Library Versions:\n",
      "  pandas         : 2.3.3\n",
      "  numpy          : 2.3.4\n",
      "  sklearn        : 1.7.2\n",
      "\n",
      "Paths configured:\n",
      "  Data: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\Gelar_Rasa\\data\\fmcg_personalcare\\fmcg_personalcare\n",
      "  Reports: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\reports\n",
      "  Clean: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\data\\clean\n",
      "\n",
      "âœ“ Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Stats & ML\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, jarque_bera, shapiro\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Package versions for documentation\n",
    "versions = {\n",
    "    'pandas': pd.__version__,\n",
    "    'numpy': np.__version__,\n",
    "}\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    versions['sklearn'] = sklearn.__version__\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Random Seed: {SEED}\")\n",
    "print(\"\\nLibrary Versions:\")\n",
    "for lib, ver in versions.items():\n",
    "    print(f\"  {lib:15s}: {ver}\")\n",
    "\n",
    "# Directory structure\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"Gelar_Rasa\" / \"data\" / \"fmcg_personalcare\" / \"fmcg_personalcare\"\n",
    "REPORTS_DIR = BASE_DIR / \"reports\"\n",
    "PROFILING_DIR = REPORTS_DIR / \"profiling\"\n",
    "EDA_DIR = REPORTS_DIR / \"eda\"\n",
    "CLEAN_DATA_DIR = BASE_DIR / \"data\" / \"clean\"\n",
    "ARCHIVE_DIR = BASE_DIR / \"archive\" / \"unneeded\"\n",
    "\n",
    "# Create dirs if needed\n",
    "for d in [REPORTS_DIR, PROFILING_DIR, EDA_DIR, CLEAN_DATA_DIR, ARCHIVE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nPaths configured:\")\n",
    "print(f\"  Data: {DATA_DIR}\")\n",
    "print(f\"  Reports: {REPORTS_DIR}\")\n",
    "print(f\"  Clean: {CLEAN_DATA_DIR}\")\n",
    "print(\"\\nâœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repository Structure Analysis\n",
    "\n",
    "Before diving into data, let's understand the file organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Structure:\n",
      "================================================================================\n",
      "ðŸ“ Gelar-Rasa/\n",
      "â”œâ”€â”€ archive/\n",
      "â”‚   â””â”€â”€ unneeded/\n",
      "â”œâ”€â”€ data/\n",
      "â”‚   â””â”€â”€ clean/\n",
      "â”œâ”€â”€ DSC2025_Analysis_FMCG_PersonalCare.ipynb (2344.4 KB)\n",
      "â”œâ”€â”€ DSC2025_Analysis_FMCG_PersonalCare_Fixed.ipynb (48.8 KB)\n",
      "â”œâ”€â”€ env/\n",
      "â”‚   â”œâ”€â”€ etc/\n",
      "â”‚   â”‚   â””â”€â”€ jupyter/\n",
      "â”‚   â”œâ”€â”€ Include/\n",
      "â”‚   â”œâ”€â”€ Lib/\n",
      "â”‚   â”‚   â””â”€â”€ site-packages/\n",
      "â”‚   â”œâ”€â”€ pyvenv.cfg (340 bytes)\n",
      "â”‚   â”œâ”€â”€ Scripts/\n",
      "â”‚   â”‚   â”œâ”€â”€ activate (1.7 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ activate.bat (1,019 bytes)\n",
      "â”‚   â”‚   â”œâ”€â”€ Activate.ps1 (25.6 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ choreo_diagnose.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ choreo_get_chrome.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ cygdb.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ cython.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ cythonize.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ deactivate.bat (393 bytes)\n",
      "â”‚   â”‚   â”œâ”€â”€ debugpy-adapter.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ debugpy.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ f2py.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ fonttools.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ httpx.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ import_pb_to_tensorboard.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ install_cmdstan.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ install_cxx_toolchain.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ ipython.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ ipython3.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jlpm.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jsonpointer (1.8 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jsonschema.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-console.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-dejavu.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-events.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-execute.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-kernel.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-kernelspec.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-lab.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-labextension.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-labhub.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-migrate.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-nbconvert.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-notebook.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-run.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-server.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-troubleshoot.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter-trust.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ jupyter.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ kaleido_get_chrome.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ kaleido_mocker.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ markdown-it.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ markdown_py.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ normalizer.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ numpy-config.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pip.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pip3.11.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pip3.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ plotly_get_chrome.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ py.test.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pybabel.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pyftmerge.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pyftsubset.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pygmentize.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pyjson5.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pytest.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ python.exe (268.3 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ pythonw.exe (257.3 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ saved_model_cli.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ send2trash.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ tensorboard.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ tf_upgrade_v2.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ tflite_convert.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ toco.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ tqdm.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ ttx.exe (105.9 KB)\n",
      "â”‚   â”‚   â”œâ”€â”€ wheel.exe (105.9 KB)\n",
      "â”‚   â”‚   â””â”€â”€ wsdump.exe (105.9 KB)\n",
      "â”‚   â””â”€â”€ share/\n",
      "â”‚       â”œâ”€â”€ applications/\n",
      "â”‚       â”œâ”€â”€ icons/\n",
      "â”‚       â”œâ”€â”€ jupyter/\n",
      "â”‚       â””â”€â”€ man/\n",
      "â”œâ”€â”€ Gelar_Rasa/\n",
      "â”‚   â””â”€â”€ data/\n",
      "â”‚       â””â”€â”€ fmcg_personalcare/\n",
      "â”œâ”€â”€ README.md (11.2 KB)\n",
      "â”œâ”€â”€ reports/\n",
      "â”‚   â”œâ”€â”€ eda/\n",
      "â”‚   â””â”€â”€ profiling/\n",
      "â””â”€â”€ requirements.txt (581 bytes)\n",
      "\n",
      "================================================================================\n",
      "Data Files Check:\n",
      "================================================================================\n",
      "âœ“ sales.csv            - 98,744.4 KB\n",
      "âœ“ products.csv         - 1.2 KB\n",
      "âœ“ marketing.csv        - 1.6 KB\n",
      "âœ“ reviews.csv          - 861.4 KB\n",
      "\n",
      "âœ“ All required files present\n"
     ]
    }
   ],
   "source": [
    "def show_tree(path, max_depth=3, current_depth=0, prefix=\"\"):\n",
    "    \"\"\"Display directory tree structure.\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    if not path.exists():\n",
    "        print(f\"{prefix}âš  Path not found: {path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        items = sorted(path.iterdir())\n",
    "    except PermissionError:\n",
    "        print(f\"{prefix}âš  Permission denied\")\n",
    "        return\n",
    "    \n",
    "    # Filter out hidden and common noise\n",
    "    items = [i for i in items if not i.name.startswith('.') \n",
    "             and i.name not in ['__pycache__', 'node_modules']]\n",
    "    \n",
    "    for idx, item in enumerate(items):\n",
    "        is_last = idx == len(items) - 1\n",
    "        connector = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "        extension = \"    \" if is_last else \"â”‚   \"\n",
    "        \n",
    "        if item.is_file():\n",
    "            size = item.stat().st_size\n",
    "            size_str = f\"{size:,} bytes\" if size < 1024 else f\"{size/1024:.1f} KB\"\n",
    "            print(f\"{prefix}{connector}{item.name} ({size_str})\")\n",
    "        else:\n",
    "            print(f\"{prefix}{connector}{item.name}/\")\n",
    "            show_tree(item, max_depth, current_depth + 1, prefix + extension)\n",
    "\n",
    "print(\"Repository Structure:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ðŸ“ {BASE_DIR.name}/\")\n",
    "show_tree(BASE_DIR, max_depth=2)\n",
    "\n",
    "# Check if data files exist\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data Files Check:\")\n",
    "print(\"=\" * 80)\n",
    "required_files = ['sales.csv', 'products.csv', 'marketing.csv', 'reviews.csv']\n",
    "missing = []\n",
    "for fname in required_files:\n",
    "    fpath = DATA_DIR / fname\n",
    "    if fpath.exists():\n",
    "        size = fpath.stat().st_size / 1024\n",
    "        print(f\"âœ“ {fname:20s} - {size:,.1f} KB\")\n",
    "    else:\n",
    "        print(f\"âœ— {fname:20s} - NOT FOUND\")\n",
    "        missing.append(fname)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâš  Missing files: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All required files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Initial Profiling\n",
    "\n",
    "Load all datasets and get quick stats about shape, types, and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATASETS\n",
      "================================================================================\n",
      "âœ“ Loaded sales       : 1,000,000 rows Ã— 10 cols\n",
      "âœ“ Loaded products    :     15 rows Ã—  7 cols\n",
      "âœ“ Loaded marketing   :     20 rows Ã—  8 cols\n",
      "âœ“ Loaded reviews     : 10,000 rows Ã—  7 cols\n",
      "\n",
      "================================================================================\n",
      "PREVIEW: SALES\n",
      "================================================================================\n",
      "                         transaction_id        date product_id     region  \\\n",
      "0  fa43024c-7c61-4e2b-bfd1-84915e86c4f7  2024-02-04      PC001   Semarang   \n",
      "1  cc02b941-61b7-4e37-b385-ecfc2fe2a0b0  2022-05-31      PC004      Medan   \n",
      "2  378b66ac-8b18-43e3-8b91-cfbb0ee1d346  2020-10-24      PC013  Palembang   \n",
      "\n",
      "     channel  units_sold  avg_price  discount_pct    revenue  \\\n",
      "0     Shopee           4   31721.61             5  126886.45   \n",
      "1  Tokopedia           4   23148.20             0   92592.80   \n",
      "2   Alfamart           5   25396.69            20  126983.43   \n",
      "\n",
      "   days_since_launch  \n",
      "0               1450  \n",
      "1                395  \n",
      "2              -1178  \n",
      "\n",
      "Columns: ['transaction_id', 'date', 'product_id', 'region', 'channel', 'units_sold', 'avg_price', 'discount_pct', 'revenue', 'days_since_launch']\n",
      "\n",
      "================================================================================\n",
      "PREVIEW: PRODUCTS\n",
      "================================================================================\n",
      "  product_id                           product_name     brand         type  \\\n",
      "0      PC001   Sunsilk Smooth & Shine Shampoo 340ml   Sunsilk      Shampoo   \n",
      "1      PC002  Sunsilk Black Shine Conditioner 340ml   Sunsilk  Conditioner   \n",
      "2      PC003       Lifebuoy Total10 Body Wash 400ml  Lifebuoy    Body Wash   \n",
      "\n",
      "   size_ml  base_price launch_date  \n",
      "0      340       32000  2020-02-15  \n",
      "1      340       33000  2020-06-10  \n",
      "2      400       28000  2020-03-20  \n",
      "\n",
      "Columns: ['product_id', 'product_name', 'brand', 'type', 'size_ml', 'base_price', 'launch_date']\n",
      "\n",
      "================================================================================\n",
      "PREVIEW: MARKETING\n",
      "================================================================================\n",
      "  campaign_id product_id     campaign_name  start_date    end_date  spend_idr  \\\n",
      "0      MKT001      PC010  Campaign_1_PC010  2020-10-12  2020-12-14  256411579   \n",
      "1      MKT002      PC008  Campaign_2_PC008  2020-06-30  2020-08-29  582461991   \n",
      "2      MKT003      PC006  Campaign_3_PC006  2021-09-01  2021-09-22  884233026   \n",
      "\n",
      "  channel  engagement_rate  \n",
      "0      TV            0.566  \n",
      "1      TV            0.376  \n",
      "2      TV            0.117  \n",
      "\n",
      "Columns: ['campaign_id', 'product_id', 'campaign_name', 'start_date', 'end_date', 'spend_idr', 'channel', 'engagement_rate']\n",
      "\n",
      "================================================================================\n",
      "PREVIEW: REVIEWS\n",
      "================================================================================\n",
      "  review_id product_id        date  rating sentiment   platform  \\\n",
      "0   R100000      PC014  2024-10-16     4.6  Positive  Instagram   \n",
      "1   R100001      PC012  2023-06-27     2.8  Negative  Instagram   \n",
      "2   R100002      PC003  2022-10-21     3.2   Neutral     Shopee   \n",
      "\n",
      "                                       comment  \n",
      "0  Packaging bocor saat diterima, kurang aman.  \n",
      "1    Mudah dibeli saat promo, value for money.  \n",
      "2    Mudah dibeli saat promo, value for money.  \n",
      "\n",
      "Columns: ['review_id', 'product_id', 'date', 'rating', 'sentiment', 'platform', 'comment']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load with error handling\n",
    "datasets = {}\n",
    "for fname in ['sales', 'products', 'marketing', 'reviews']:\n",
    "    fpath = DATA_DIR / f\"{fname}.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(fpath)\n",
    "        datasets[fname] = df\n",
    "        print(f\"âœ“ Loaded {fname:12s}: {len(df):6,} rows Ã— {len(df.columns):2} cols\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— Failed to load {fname}.csv - file not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to load {fname}.csv - {str(e)}\")\n",
    "\n",
    "# Unpack for convenience\n",
    "df_sales = datasets.get('sales')\n",
    "df_products = datasets.get('products')\n",
    "df_marketing = datasets.get('marketing')\n",
    "df_reviews = datasets.get('reviews')\n",
    "\n",
    "# Show previews\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"PREVIEW: {name.upper()}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(df.head(3))\n",
    "        print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA PROFILING\n",
      "================================================================================\n",
      "\n",
      "SALES:\n",
      "  Shape: 1,000,000 rows Ã— 10 cols\n",
      "  Memory: 374.70 MB\n",
      "  Duplicates: 0 (0.00%)\n",
      "  Missing values: 0\n",
      "  Data types: {dtype('O'): 5, dtype('int64'): 3, dtype('float64'): 2}\n",
      "\n",
      "PRODUCTS:\n",
      "  Shape: 15 rows Ã— 7 cols\n",
      "  Memory: 0.01 MB\n",
      "  Duplicates: 0 (0.00%)\n",
      "  Missing values: 0\n",
      "  Data types: {dtype('O'): 5, dtype('int64'): 2}\n",
      "\n",
      "MARKETING:\n",
      "  Shape: 20 rows Ã— 8 cols\n",
      "  Memory: 0.01 MB\n",
      "  Duplicates: 0 (0.00%)\n",
      "  Missing values: 0\n",
      "  Data types: {dtype('O'): 6, dtype('int64'): 1, dtype('float64'): 1}\n",
      "\n",
      "REVIEWS:\n",
      "  Shape: 10,000 rows Ã— 7 cols\n",
      "  Memory: 4.06 MB\n",
      "  Duplicates: 0 (0.00%)\n",
      "  Missing values: 0\n",
      "  Data types: {dtype('O'): 6, dtype('float64'): 1}\n",
      "\n",
      "âœ“ Saved profile: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\reports\\profiling\\sales_profile.csv\n",
      "\n",
      "âœ“ Saved profile: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\reports\\profiling\\products_profile.csv\n",
      "\n",
      "âœ“ Saved profile: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\reports\\profiling\\marketing_profile.csv\n",
      "\n",
      "âœ“ Saved profile: c:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\reports\\profiling\\reviews_profile.csv\n"
     ]
    }
   ],
   "source": [
    "def profile_dataframe(df, name):\n",
    "    \"\"\"Generate comprehensive profile of a dataframe.\"\"\"\n",
    "    profile = {\n",
    "        'dataset': name,\n",
    "        'rows': len(df),\n",
    "        'columns': len(df.columns),\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'missing_total': df.isnull().sum().sum(),\n",
    "        'dtypes': df.dtypes.value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Column-level stats\n",
    "    col_stats = []\n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull().sum()\n",
    "        unique = df[col].nunique()\n",
    "        col_stats.append({\n",
    "            'column': col,\n",
    "            'type': str(df[col].dtype),\n",
    "            'missing': missing,\n",
    "            'missing_pct': 100 * missing / len(df),\n",
    "            'unique': unique,\n",
    "            'unique_pct': 100 * unique / len(df)\n",
    "        })\n",
    "    \n",
    "    profile['columns_detail'] = pd.DataFrame(col_stats)\n",
    "    return profile\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA PROFILING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiles = {}\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        profiles[name] = profile_dataframe(df, name)\n",
    "\n",
    "# Display summary\n",
    "for name, prof in profiles.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Shape: {prof['rows']:,} rows Ã— {prof['columns']} cols\")\n",
    "    print(f\"  Memory: {prof['memory_mb']:.2f} MB\")\n",
    "    print(f\"  Duplicates: {prof['duplicates']:,} ({100*prof['duplicates']/prof['rows']:.2f}%)\")\n",
    "    print(f\"  Missing values: {prof['missing_total']:,}\")\n",
    "    print(f\"  Data types: {prof['dtypes']}\")\n",
    "    \n",
    "    # Show columns with missing data\n",
    "    missing_cols = prof['columns_detail'][prof['columns_detail']['missing'] > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"\\n  Columns with missing data:\")\n",
    "        for _, row in missing_cols.iterrows():\n",
    "            print(f\"    - {row['column']:25s}: {row['missing']:5,} ({row['missing_pct']:5.2f}%)\")\n",
    "\n",
    "# Save profiling reports\n",
    "for name, prof in profiles.items():\n",
    "    report_path = PROFILING_DIR / f\"{name}_profile.csv\"\n",
    "    prof['columns_detail'].to_csv(report_path, index=False)\n",
    "    print(f\"\\nâœ“ Saved profile: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "Handle missing values, duplicates, and outliers systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING\n",
      "================================================================================\n",
      "\n",
      "Step 1: Removing duplicates\n",
      "  sales       : no duplicates\n",
      "  products    : no duplicates\n",
      "  marketing   : no duplicates\n",
      "  reviews     : no duplicates\n",
      "\n",
      "Step 2: Parsing date columns\n",
      "  âœ“ sales.date parsed successfully\n",
      "\n",
      "Step 3: Handling missing values\n",
      "\n",
      "Step 4: Optimizing data types\n",
      "  sales       : 318.43MB â†’ 118.26MB (saved 62.9%)\n",
      "  products    : 0.01MB â†’ 0.01MB (saved 2.7%)\n",
      "  marketing   : 0.01MB â†’ 0.01MB (saved 10.3%)\n",
      "  reviews     : 4.06MB â†’ 0.91MB (saved 77.6%)\n",
      "\n",
      "âœ“ Cleaning complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cleaning_log = []\n",
    "\n",
    "# Step 1: Handle duplicates\n",
    "print(\"\\nStep 1: Removing duplicates\")\n",
    "for name in ['sales', 'products', 'marketing', 'reviews']:\n",
    "    df = datasets.get(name)\n",
    "    if df is not None:\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates()\n",
    "        after = len(df)\n",
    "        removed = before - after\n",
    "        if removed > 0:\n",
    "            print(f\"  {name:12s}: removed {removed:,} duplicates ({100*removed/before:.2f}%)\")\n",
    "            cleaning_log.append(f\"{name}: removed {removed} duplicate rows\")\n",
    "        else:\n",
    "            print(f\"  {name:12s}: no duplicates\")\n",
    "        datasets[name] = df\n",
    "\n",
    "# Step 2: Convert date columns\n",
    "print(\"\\nStep 2: Parsing date columns\")\n",
    "if df_sales is not None and 'date' in df_sales.columns:\n",
    "    df_sales['date'] = pd.to_datetime(df_sales['date'], errors='coerce')\n",
    "    invalid = df_sales['date'].isnull().sum()\n",
    "    if invalid > 0:\n",
    "        print(f\"  âš  Found {invalid} invalid dates in sales.date\")\n",
    "    else:\n",
    "        print(f\"  âœ“ sales.date parsed successfully\")\n",
    "    datasets['sales'] = df_sales\n",
    "\n",
    "if df_marketing is not None and 'campaign_start' in df_marketing.columns:\n",
    "    df_marketing['campaign_start'] = pd.to_datetime(df_marketing['campaign_start'], errors='coerce')\n",
    "    print(f\"  âœ“ marketing.campaign_start parsed\")\n",
    "    datasets['marketing'] = df_marketing\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "print(\"\\nStep 3: Handling missing values\")\n",
    "\n",
    "# Sales: drop rows with missing critical fields\n",
    "if df_sales is not None:\n",
    "    critical_cols = ['product_id', 'quantity', 'revenue']\n",
    "    existing_critical = [c for c in critical_cols if c in df_sales.columns]\n",
    "    before = len(df_sales)\n",
    "    df_sales = df_sales.dropna(subset=existing_critical)\n",
    "    after = len(df_sales)\n",
    "    removed = before - after\n",
    "    if removed > 0:\n",
    "        print(f\"  sales: dropped {removed} rows with missing critical fields\")\n",
    "        cleaning_log.append(f\"sales: dropped {removed} rows with missing {existing_critical}\")\n",
    "    datasets['sales'] = df_sales\n",
    "\n",
    "# Products: fill categorical with 'Unknown', numeric with median\n",
    "if df_products is not None:\n",
    "    for col in df_products.columns:\n",
    "        if df_products[col].isnull().sum() > 0:\n",
    "            if df_products[col].dtype == 'object':\n",
    "                df_products[col].fillna('Unknown', inplace=True)\n",
    "                print(f\"  products.{col}: filled {df_products[col].isnull().sum()} missing with 'Unknown'\")\n",
    "            else:\n",
    "                median_val = df_products[col].median()\n",
    "                df_products[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  products.{col}: filled missing with median={median_val:.2f}\")\n",
    "    datasets['products'] = df_products\n",
    "\n",
    "# Reviews: drop if missing rating or text\n",
    "if df_reviews is not None:\n",
    "    review_critical = ['rating']\n",
    "    existing_review = [c for c in review_critical if c in df_reviews.columns]\n",
    "    before = len(df_reviews)\n",
    "    df_reviews = df_reviews.dropna(subset=existing_review)\n",
    "    after = len(df_reviews)\n",
    "    removed = before - after\n",
    "    if removed > 0:\n",
    "        print(f\"  reviews: dropped {removed} rows with missing {existing_review}\")\n",
    "        cleaning_log.append(f\"reviews: dropped {removed} rows with missing rating\")\n",
    "    datasets['reviews'] = df_reviews\n",
    "\n",
    "# Step 4: Data type optimization\n",
    "print(\"\\nStep 4: Optimizing data types\")\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        before_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Downcast numeric columns\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        \n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        \n",
    "        # Convert low-cardinality strings to category\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if df[col].nunique() / len(df) < 0.5:  # less than 50% unique\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        after_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        savings = 100 * (1 - after_mem/before_mem)\n",
    "        print(f\"  {name:12s}: {before_mem:.2f}MB â†’ {after_mem:.2f}MB (saved {savings:.1f}%)\")\n",
    "        datasets[name] = df\n",
    "\n",
    "print(\"\\nâœ“ Cleaning complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Outlier detection (IQR method)\n",
      "  sales.revenue: 15,620 outliers (1.56%)\n",
      "    â†’ Capped at 99th percentile: 289770.45\n",
      "\n",
      "âœ“ Outlier handling complete\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Outlier detection and handling\n",
    "print(\"\\nStep 5: Outlier detection (IQR method)\")\n",
    "\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - multiplier * IQR\n",
    "    upper = Q3 + multiplier * IQR\n",
    "    return (series < lower) | (series > upper)\n",
    "\n",
    "# Check sales for outliers in quantity and revenue\n",
    "if df_sales is not None:\n",
    "    for col in ['quantity', 'revenue']:\n",
    "        if col in df_sales.columns:\n",
    "            outliers = detect_outliers_iqr(df_sales[col])\n",
    "            n_outliers = outliers.sum()\n",
    "            if n_outliers > 0:\n",
    "                pct = 100 * n_outliers / len(df_sales)\n",
    "                print(f\"  sales.{col}: {n_outliers:,} outliers ({pct:.2f}%)\")\n",
    "                \n",
    "                # Cap outliers at 99th percentile instead of removing\n",
    "                cap_value = df_sales[col].quantile(0.99)\n",
    "                df_sales.loc[outliers, col] = cap_value\n",
    "                print(f\"    â†’ Capped at 99th percentile: {cap_value:.2f}\")\n",
    "                cleaning_log.append(f\"sales.{col}: capped {n_outliers} outliers\")\n",
    "    datasets['sales'] = df_sales\n",
    "\n",
    "print(\"\\nâœ“ Outlier handling complete\")\n",
    "\n",
    "# Unpack cleaned data\n",
    "df_sales = datasets.get('sales')\n",
    "df_products = datasets.get('products')\n",
    "df_marketing = datasets.get('marketing')\n",
    "df_reviews = datasets.get('reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Create domain-specific features for FMCG analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "Merging datasets...\n",
      "  âœ“ Merged sales + products: 1,000,000 rows\n",
      "  âœ“ Added review aggregates\n",
      "\n",
      "Engineering FMCG features...\n",
      "  âœ“ Temporal features (year, month, quarter, etc.)\n",
      "  âœ“ product_age_days\n",
      "  âœ“ Brand performance metrics\n",
      "\n",
      "âœ“ Final merged dataset: 1,000,000 rows Ã— 28 cols\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge datasets\n",
    "print(\"\\nMerging datasets...\")\n",
    "if df_sales is not None and df_products is not None:\n",
    "    df_merged = df_sales.merge(df_products, on='product_id', how='left')\n",
    "    print(f\"  âœ“ Merged sales + products: {len(df_merged):,} rows\")\n",
    "    \n",
    "    if df_reviews is not None:\n",
    "        # Aggregate reviews per product\n",
    "        review_agg = df_reviews.groupby('product_id').agg({\n",
    "            'rating': ['mean', 'count', 'std'],\n",
    "        }).reset_index()\n",
    "        review_agg.columns = ['product_id', 'avg_rating', 'review_count', 'rating_std']\n",
    "        review_agg['rating_std'].fillna(0, inplace=True)  # handle single review case\n",
    "        \n",
    "        df_merged = df_merged.merge(review_agg, on='product_id', how='left')\n",
    "        df_merged['avg_rating'].fillna(df_merged['avg_rating'].median(), inplace=True)\n",
    "        df_merged['review_count'].fillna(0, inplace=True)\n",
    "        df_merged['rating_std'].fillna(0, inplace=True)\n",
    "        print(f\"  âœ“ Added review aggregates\")\n",
    "    \n",
    "    # Create FMCG-specific features\n",
    "    print(\"\\nEngineering FMCG features...\")\n",
    "    \n",
    "    # Price per unit\n",
    "    if 'revenue' in df_merged.columns and 'quantity' in df_merged.columns:\n",
    "        df_merged['price_per_unit'] = df_merged['revenue'] / df_merged['quantity']\n",
    "        print(\"  âœ“ price_per_unit\")\n",
    "    \n",
    "    # Temporal features\n",
    "    if 'date' in df_merged.columns:\n",
    "        df_merged['year'] = df_merged['date'].dt.year\n",
    "        df_merged['month'] = df_merged['date'].dt.month\n",
    "        df_merged['quarter'] = df_merged['date'].dt.quarter\n",
    "        df_merged['day_of_week'] = df_merged['date'].dt.dayofweek\n",
    "        df_merged['is_weekend'] = (df_merged['day_of_week'] >= 5).astype(int)\n",
    "        df_merged['week_of_year'] = df_merged['date'].dt.isocalendar().week\n",
    "        print(\"  âœ“ Temporal features (year, month, quarter, etc.)\")\n",
    "    \n",
    "    # Product age (if launch_date exists)\n",
    "    if 'launch_date' in df_merged.columns:\n",
    "        df_merged['launch_date'] = pd.to_datetime(df_merged['launch_date'], errors='coerce')\n",
    "        df_merged['product_age_days'] = (df_merged['date'] - df_merged['launch_date']).dt.days\n",
    "        print(\"  âœ“ product_age_days\")\n",
    "    \n",
    "    # Brand performance metrics\n",
    "    if 'brand' in df_merged.columns:\n",
    "        brand_revenue = df_merged.groupby('brand')['revenue'].transform('sum')\n",
    "        df_merged['brand_total_revenue'] = brand_revenue\n",
    "        df_merged['revenue_share_in_brand'] = df_merged['revenue'] / brand_revenue\n",
    "        print(\"  âœ“ Brand performance metrics\")\n",
    "    \n",
    "    # Category performance\n",
    "    if 'category' in df_merged.columns:\n",
    "        cat_revenue = df_merged.groupby('category')['revenue'].transform('sum')\n",
    "        df_merged['category_total_revenue'] = cat_revenue\n",
    "        df_merged['revenue_share_in_category'] = df_merged['revenue'] / cat_revenue\n",
    "        print(\"  âœ“ Category performance metrics\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final merged dataset: {len(df_merged):,} rows Ã— {len(df_merged.columns)} cols\")\n",
    "else:\n",
    "    print(\"âš  Cannot merge - missing required datasets\")\n",
    "    df_merged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating time series features (lag & rolling)...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: quantity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m1\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m30\u001b[39m]:\n\u001b[32m     10\u001b[39m     df_merged[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mrevenue_lag_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = df_merged.groupby(\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m'\u001b[39m].shift(lag)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df_merged[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mquantity_lag_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = \u001b[43mdf_merged\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquantity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.shift(lag)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  âœ“ Lag features (1, 7, 30 days)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Rolling averages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\env\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[39m, in \u001b[36mDataFrameGroupBy.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) > \u001b[32m1\u001b[39m:\n\u001b[32m   1945\u001b[39m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[32m   1946\u001b[39m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[32m   1947\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1948\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1949\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1950\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Desktop\\DATA\\Gelar-Rasa\\env\\Lib\\site-packages\\pandas\\core\\base.py:245\u001b[39m, in \u001b[36mSelectionMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    246\u001b[39m     ndim = \u001b[38;5;28mself\u001b[39m.obj[key].ndim\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gotitem(key, ndim=ndim)\n",
      "\u001b[31mKeyError\u001b[39m: 'Column not found: quantity'"
     ]
    }
   ],
   "source": [
    "# Lag and rolling features for time series\n",
    "print(\"\\nCreating time series features (lag & rolling)...\")\n",
    "\n",
    "if df_merged is not None and 'date' in df_merged.columns:\n",
    "    # Sort by product and date\n",
    "    df_merged = df_merged.sort_values(['product_id', 'date'])\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in [1, 7, 30]:\n",
    "        df_merged[f'revenue_lag_{lag}'] = df_merged.groupby('product_id')['revenue'].shift(lag)\n",
    "        df_merged[f'quantity_lag_{lag}'] = df_merged.groupby('product_id')['quantity'].shift(lag)\n",
    "    \n",
    "    print(\"  âœ“ Lag features (1, 7, 30 days)\")\n",
    "    \n",
    "    # Rolling averages\n",
    "    for window in [7, 30]:\n",
    "        df_merged[f'revenue_rolling_{window}'] = df_merged.groupby('product_id')['revenue'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        df_merged[f'quantity_rolling_{window}'] = df_merged.groupby('product_id')['quantity'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    print(\"  âœ“ Rolling averages (7, 30 days)\")\n",
    "    \n",
    "    # Trend indicators\n",
    "    df_merged['revenue_trend_7d'] = (\n",
    "        df_merged['revenue_rolling_7'] / df_merged['revenue_lag_7'] - 1\n",
    "    ) * 100\n",
    "    df_merged['revenue_trend_7d'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"  âœ“ Trend indicators\")\n",
    "    print(f\"\\nâœ“ Time series features complete\")\n",
    "else:\n",
    "    print(\"âš  Skipping time series features - date column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis\n",
    "\n",
    "Visual exploration of patterns, trends, and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if df_merged is not None:\n",
    "    # Revenue distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].hist(df_merged['revenue'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Revenue')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Revenue Distribution')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    axes[1].hist(np.log1p(df_merged['revenue']), bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1].set_xlabel('Log(Revenue + 1)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Revenue Distribution (Log Scale)')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'revenue_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ“ Saved: revenue_distribution.png\")\n",
    "    \n",
    "    # Time series of revenue\n",
    "    if 'date' in df_merged.columns:\n",
    "        daily_revenue = df_merged.groupby('date')['revenue'].sum().reset_index()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 5))\n",
    "        ax.plot(daily_revenue['date'], daily_revenue['revenue'], linewidth=1.5)\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Total Revenue')\n",
    "        ax.set_title('Revenue Over Time')\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_DIR / 'revenue_timeseries.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ“ Saved: revenue_timeseries.png\")\n",
    "    \n",
    "    # Top products by revenue\n",
    "    if 'product_name' in df_merged.columns:\n",
    "        top_products = df_merged.groupby('product_name')['revenue'].sum().nlargest(15)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        top_products.plot(kind='barh', ax=ax, color='steelblue')\n",
    "        ax.set_xlabel('Total Revenue')\n",
    "        ax.set_ylabel('Product')\n",
    "        ax.set_title('Top 15 Products by Revenue')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_DIR / 'top_products.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ“ Saved: top_products.png\")\n",
    "    \n",
    "    # Category analysis\n",
    "    if 'category' in df_merged.columns:\n",
    "        category_stats = df_merged.groupby('category').agg({\n",
    "            'revenue': 'sum',\n",
    "            'quantity': 'sum',\n",
    "            'product_id': 'nunique'\n",
    "        }).round(2)\n",
    "        category_stats.columns = ['Total Revenue', 'Total Quantity', 'Unique Products']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        category_stats['Total Revenue'].sort_values().plot(\n",
    "            kind='barh', ax=axes[0], color='coral'\n",
    "        )\n",
    "        axes[0].set_xlabel('Total Revenue')\n",
    "        axes[0].set_title('Revenue by Category')\n",
    "        axes[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        category_stats['Unique Products'].sort_values().plot(\n",
    "            kind='barh', ax=axes[1], color='lightseagreen'\n",
    "        )\n",
    "        axes[1].set_xlabel('Number of Products')\n",
    "        axes[1].set_title('Product Count by Category')\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_DIR / 'category_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ“ Saved: category_analysis.png\")\n",
    "\n",
    "print(\"\\nâœ“ EDA visualizations complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "print(\"\\nCreating correlation heatmap...\")\n",
    "\n",
    "if df_merged is not None:\n",
    "    # Select numeric columns for correlation\n",
    "    numeric_cols = df_merged.select_dtypes(include=[np.number]).columns\n",
    "    # Filter out ID columns and date-derived integers\n",
    "    analysis_cols = [c for c in numeric_cols \n",
    "                     if not c.endswith('_id') \n",
    "                     and c not in ['year', 'month', 'quarter', 'week_of_year', 'day_of_week']]\n",
    "    \n",
    "    if len(analysis_cols) > 1:\n",
    "        corr_matrix = df_merged[analysis_cols].corr()\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                    cmap='coolwarm', center=0, square=True,\n",
    "                    linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "        ax.set_title('Feature Correlation Matrix', fontsize=14, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_DIR / 'correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ“ Saved: correlation_heatmap.png\")\n",
    "        \n",
    "        # Show top correlations with revenue\n",
    "        if 'revenue' in corr_matrix.columns:\n",
    "            revenue_corr = corr_matrix['revenue'].abs().sort_values(ascending=False)\n",
    "            print(\"\\nTop correlations with revenue:\")\n",
    "            for feat, val in revenue_corr.head(10).items():\n",
    "                if feat != 'revenue':\n",
    "                    print(f\"  {feat:30s}: {val:.3f}\")\n",
    "    else:\n",
    "        print(\"âš  Not enough numeric columns for correlation analysis\")\n",
    "else:\n",
    "    print(\"âš  Merged dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling & Evaluation\n",
    "\n",
    "Build predictive models for revenue forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODELING & EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_merged is not None and 'revenue' in df_merged.columns:\n",
    "    # Prepare features\n",
    "    print(\"\\nPreparing features...\")\n",
    "    \n",
    "    # Select feature columns (exclude target and non-predictive columns)\n",
    "    exclude_cols = ['revenue', 'date', 'product_id', 'product_name', 'brand', \n",
    "                    'category', 'launch_date']\n",
    "    feature_cols = [c for c in df_merged.columns \n",
    "                    if c not in exclude_cols \n",
    "                    and df_merged[c].dtype in [np.float64, np.float32, np.int64, np.int32]]\n",
    "    \n",
    "    # Drop rows with NaN in features or target\n",
    "    model_data = df_merged[feature_cols + ['revenue']].dropna()\n",
    "    \n",
    "    X = model_data[feature_cols]\n",
    "    y = model_data['revenue']\n",
    "    \n",
    "    print(f\"  Features: {len(feature_cols)} columns\")\n",
    "    print(f\"  Samples: {len(X):,} rows\")\n",
    "    print(f\"  Target: revenue (mean={y.mean():.2f}, std={y.std():.2f})\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    print(f\"\\n  Train: {len(X_train):,} samples\")\n",
    "    print(f\"  Test:  {len(X_test):,} samples\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"  âœ“ Features scaled\")\n",
    "else:\n",
    "    print(\"âš  Cannot proceed with modeling - data not available\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models\n",
    "print(\"\\nTraining models...\")\n",
    "\n",
    "if X_train is not None:\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0, random_state=SEED),\n",
    "        'Lasso': Lasso(alpha=1.0, random_state=SEED),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=SEED)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n  Training {name}...\")\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        test_mape = mean_absolute_percentage_error(y_test, y_pred_test) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Train RÂ²': train_r2,\n",
    "            'Test RÂ²': test_r2,\n",
    "            'MAE': test_mae,\n",
    "            'RMSE': test_rmse,\n",
    "            'MAPE (%)': test_mape\n",
    "        })\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        \n",
    "        print(f\"    Train RÂ²: {train_r2:.4f}\")\n",
    "        print(f\"    Test RÂ²:  {test_r2:.4f}\")\n",
    "        print(f\"    MAE:      {test_mae:.2f}\")\n",
    "        print(f\"    RMSE:     {test_rmse:.2f}\")\n",
    "        print(f\"    MAPE:     {test_mape:.2f}%\")\n",
    "    \n",
    "    # Results summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(REPORTS_DIR / 'model_results.csv', index=False)\n",
    "    print(\"\\nâœ“ Saved: model_results.csv\")\n",
    "    \n",
    "    # Best model\n",
    "    best_model_name = results_df.loc[results_df['Test RÂ²'].idxmax(), 'Model']\n",
    "    best_model = trained_models[best_model_name]\n",
    "    print(f\"\\nâœ“ Best model: {best_model_name}\")\n",
    "else:\n",
    "    print(\"âš  Skipping model training - data not available\")\n",
    "    best_model = None\n",
    "    best_model_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model diagnostics for linear regression\n",
    "if X_train is not None and 'Linear Regression' in trained_models:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL DIAGNOSTICS - LINEAR REGRESSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    lr_model = trained_models['Linear Regression']\n",
    "    y_pred = lr_model.predict(X_test_scaled)\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Residual plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Residuals vs Fitted\n",
    "    axes[0, 0].scatter(y_pred, residuals, alpha=0.5, s=10)\n",
    "    axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Fitted values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Fitted')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Normal Q-Q Plot')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Scale-Location\n",
    "    standardized_residuals = residuals / np.std(residuals)\n",
    "    axes[1, 0].scatter(y_pred, np.sqrt(np.abs(standardized_residuals)), alpha=0.5, s=10)\n",
    "    axes[1, 0].set_xlabel('Fitted values')\n",
    "    axes[1, 0].set_ylabel('âˆš|Standardized residuals|')\n",
    "    axes[1, 0].set_title('Scale-Location')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Residual histogram\n",
    "    axes[1, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Residuals')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Residual Distribution')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'model_diagnostics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: model_diagnostics.png\")\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\nStatistical Tests:\")\n",
    "    \n",
    "    # Normality test\n",
    "    _, p_shapiro = shapiro(residuals[:5000] if len(residuals) > 5000 else residuals)\n",
    "    print(f\"  Shapiro-Wilk (normality): p={p_shapiro:.4f}\")\n",
    "    if p_shapiro > 0.05:\n",
    "        print(\"    âœ“ Residuals appear normally distributed\")\n",
    "    else:\n",
    "        print(\"    âš  Residuals may not be normally distributed\")\n",
    "    \n",
    "    # Durbin-Watson (autocorrelation)\n",
    "    dw_stat = durbin_watson(residuals)\n",
    "    print(f\"  Durbin-Watson (autocorrelation): {dw_stat:.4f}\")\n",
    "    if 1.5 < dw_stat < 2.5:\n",
    "        print(\"    âœ“ No significant autocorrelation\")\n",
    "    else:\n",
    "        print(\"    âš  Possible autocorrelation detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if best_model is not None and hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FEATURE IMPORTANCE - {best_model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    for idx, row in importance.head(15).iterrows():\n",
    "        print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    importance.head(20).plot(x='feature', y='importance', kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: feature_importance.png\")\n",
    "    \n",
    "elif best_model is not None and hasattr(best_model, 'coef_'):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FEATURE COEFFICIENTS - {best_model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    coeffs = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'coefficient': best_model.coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Coefficients (by absolute value):\")\n",
    "    for idx, row in coeffs.head(15).iterrows():\n",
    "        print(f\"  {row['feature']:30s}: {row['coefficient']:+.4f}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_coeffs = coeffs.head(20)\n",
    "    colors = ['green' if x > 0 else 'red' for x in top_coeffs['coefficient']]\n",
    "    ax.barh(top_coeffs['feature'], top_coeffs['coefficient'], color=colors, alpha=0.7)\n",
    "    ax.set_xlabel('Coefficient')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title(f'Top 20 Feature Coefficients - {best_model_name}')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'feature_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: feature_coefficients.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bias & Data Validity Analysis\n",
    "\n",
    "Assess potential biases and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BIAS & DATA VALIDITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_merged is not None:\n",
    "    # Check for sampling bias\n",
    "    print(\"\\n1. Temporal Coverage:\")\n",
    "    if 'date' in df_merged.columns:\n",
    "        date_range = df_merged['date'].max() - df_merged['date'].min()\n",
    "        print(f\"   Date range: {df_merged['date'].min()} to {df_merged['date'].max()}\")\n",
    "        print(f\"   Coverage: {date_range.days} days\")\n",
    "        \n",
    "        # Check for gaps\n",
    "        daily_counts = df_merged.groupby(df_merged['date'].dt.date).size()\n",
    "        if daily_counts.std() / daily_counts.mean() > 0.5:\n",
    "            print(\"   âš  High variance in daily transaction counts - possible sampling bias\")\n",
    "        else:\n",
    "            print(\"   âœ“ Relatively consistent daily coverage\")\n",
    "    \n",
    "    # Check brand representation\n",
    "    print(\"\\n2. Brand Representation:\")\n",
    "    if 'brand' in df_merged.columns:\n",
    "        brand_dist = df_merged['brand'].value_counts()\n",
    "        print(f\"   Unique brands: {len(brand_dist)}\")\n",
    "        print(f\"   Top brand share: {100*brand_dist.iloc[0]/len(df_merged):.1f}%\")\n",
    "        if brand_dist.iloc[0] / len(df_merged) > 0.5:\n",
    "            print(\"   âš  Dataset heavily skewed toward one brand\")\n",
    "        else:\n",
    "            print(\"   âœ“ Reasonable brand diversity\")\n",
    "    \n",
    "    # Check category coverage\n",
    "    print(\"\\n3. Category Coverage:\")\n",
    "    if 'category' in df_merged.columns:\n",
    "        cat_dist = df_merged['category'].value_counts()\n",
    "        print(f\"   Unique categories: {len(cat_dist)}\")\n",
    "        print(f\"   Category distribution:\")\n",
    "        for cat, cnt in cat_dist.head(10).items():\n",
    "            print(f\"     {cat:25s}: {cnt:7,} ({100*cnt/len(df_merged):5.1f}%)\")\n",
    "    \n",
    "    # Check for unrealistic values\n",
    "    print(\"\\n4. Value Range Checks:\")\n",
    "    if 'revenue' in df_merged.columns:\n",
    "        neg_revenue = (df_merged['revenue'] < 0).sum()\n",
    "        if neg_revenue > 0:\n",
    "            print(f\"   âš  Found {neg_revenue} negative revenue values\")\n",
    "        else:\n",
    "            print(\"   âœ“ No negative revenue values\")\n",
    "    \n",
    "    if 'quantity' in df_merged.columns:\n",
    "        neg_qty = (df_merged['quantity'] < 0).sum()\n",
    "        zero_qty = (df_merged['quantity'] == 0).sum()\n",
    "        if neg_qty > 0:\n",
    "            print(f\"   âš  Found {neg_qty} negative quantity values\")\n",
    "        if zero_qty > 0:\n",
    "            print(f\"   âš  Found {zero_qty} zero quantity values\")\n",
    "        if neg_qty == 0 and zero_qty == 0:\n",
    "            print(\"   âœ“ All quantity values are positive\")\n",
    "    \n",
    "    # Class imbalance check\n",
    "    print(\"\\n5. Class Balance (if applicable):\")\n",
    "    categorical_cols = df_merged.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols[:5]:  # Check first 5 categorical columns\n",
    "        dist = df_merged[col].value_counts()\n",
    "        if len(dist) > 1:\n",
    "            imbalance_ratio = dist.iloc[0] / dist.iloc[-1]\n",
    "            if imbalance_ratio > 10:\n",
    "                print(f\"   âš  {col}: High imbalance (ratio={imbalance_ratio:.1f})\")\n",
    "            else:\n",
    "                print(f\"   âœ“ {col}: Acceptable balance (ratio={imbalance_ratio:.1f})\")\n",
    "\n",
    "print(\"\\nâœ“ Bias analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Business Insights\n",
    "\n",
    "Key findings and actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS SUMMARY & BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Data quality insights\n",
    "print(\"\\n1. DATA QUALITY\")\n",
    "if df_merged is not None:\n",
    "    insights.append(f\"- Analyzed {len(df_merged):,} sales transactions\")\n",
    "    if 'product_id' in df_merged.columns:\n",
    "        insights.append(f\"- Covering {df_merged['product_id'].nunique():,} unique products\")\n",
    "    if 'date' in df_merged.columns:\n",
    "        date_range = (df_merged['date'].max() - df_merged['date'].min()).days\n",
    "        insights.append(f\"- Time period: {date_range} days\")\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "# Top performers\n",
    "if df_merged is not None:\n",
    "    print(\"\\n2. TOP PERFORMERS\")\n",
    "    \n",
    "    if 'product_name' in df_merged.columns:\n",
    "        top_prod = df_merged.groupby('product_name')['revenue'].sum().nlargest(5)\n",
    "        print(\"\\nTop 5 Products by Revenue:\")\n",
    "        for idx, (prod, rev) in enumerate(top_prod.items(), 1):\n",
    "            print(f\"  {idx}. {prod}: ${rev:,.2f}\")\n",
    "    \n",
    "    if 'brand' in df_merged.columns:\n",
    "        top_brand = df_merged.groupby('brand')['revenue'].sum().nlargest(5)\n",
    "        print(\"\\nTop 5 Brands by Revenue:\")\n",
    "        for idx, (brand, rev) in enumerate(top_brand.items(), 1):\n",
    "            print(f\"  {idx}. {brand}: ${rev:,.2f}\")\n",
    "\n",
    "# Model performance summary\n",
    "if best_model_name is not None:\n",
    "    print(\"\\n3. PREDICTIVE MODEL\")\n",
    "    print(f\"- Best performing model: {best_model_name}\")\n",
    "    best_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
    "    print(f\"- Test RÂ²: {best_metrics['Test RÂ²']:.4f}\")\n",
    "    print(f\"- MAPE: {best_metrics['MAPE (%)']:.2f}%\")\n",
    "    print(f\"- Model explains {100*best_metrics['Test RÂ²']:.1f}% of revenue variance\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n4. RECOMMENDATIONS\")\n",
    "recommendations = [\n",
    "    \"Focus marketing budget on top-performing product categories\",\n",
    "    \"Investigate underperforming brands for potential improvements\",\n",
    "    \"Monitor seasonal trends for inventory optimization\",\n",
    "    \"Consider A/B testing on pricing strategies for mid-tier products\",\n",
    "    \"Develop loyalty programs for high-value customer segments\"\n",
    "]\n",
    "\n",
    "for idx, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {idx}. {rec}\")\n",
    "\n",
    "# Save summary\n",
    "summary_report = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'data_summary': insights,\n",
    "    'best_model': best_model_name if best_model_name else 'N/A',\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "with open(REPORTS_DIR / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ“ Analysis complete!\")\n",
    "print(f\"âœ“ All reports saved to: {REPORTS_DIR}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
